{
  "hash": "6cc699d1f487f708596c6c7ea8584802",
  "result": {
    "markdown": "---\ntitle: \"Calculus\"\neditor: visual\ncode-fold: true\nfig-width: 5\nfig-height: 3\n---\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Packages\"}\nlibrary(ggplot2)\ntheme_set(\n  theme_minimal(base_family = \"Amiri\") +\n  theme(axis.title.y = element_text(angle = 0, vjust = 1/2))\n)\n```\n:::\n\n\n## Derivatives\n\n**Limit Definition**\n\n$$\n\\begin{align}\nf'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}, &&\\text{provided the limit exists}\n\\end{align}\n$$ {#eq-deriv-limit}\n\nDerivatives are used to understand instantaneous rates of change, i.e., when $\\Delta x \\approx 0$.\n\nThis formula is equivalent:\n\n$$\nf^\\prime (a) = \\lim_{b \\to a} \\frac{f(b) - f(a)}{b-a}\n$$\n\nFor example, the area of a growing square is $A(x) = x^2$. How does $A(x)$ change every time $x$ changes? We have that, for any given value of $x$, the rate of instantaneous change is given by:\n\n$$\n\\begin{align}\nA^\\prime (x) = 2x && \\text{or} && \\frac{dA}{dx} = 2x\n\\end{align}\n$$\n\n*Note. When* $\\Delta$ *is infinitely small we change notation to* $d$*.*\n\nIf we zoom in on *any* function at a given point $a$, a tangent line at $a$ will be given by the following formula:\n\n$$\ny = f(a) + f^\\prime (a) (x-a)\n$$\n\n<aside>*linear approximation of* $f$ *near* $x=a$</aside>\n\n**Critical points**\n\n$c$ is a critical point for the smooth function $f$ iff $f^\\prime(c) = 0$.\n\nIn order to figure out if $c$ is a local maximum or local minimum, we use the *second derivative.* The second derivative of $f$ measures the rate of change of the first derivative. In a nutshell, we find critical points by solving $f^\\prime = 0$. We then plug these values into $f^{\\prime\\prime}$. If the result is positive, the critical point is a local minimum; if the result is negative, the critical point is a local maximum.\n\n*This procedure does not always work.*\n\n*Note. For higher order derivatives we use the* $f^{(n)}(x)$ *notation, where* $f^{(0)}(x) = f(x)$*. We can also use the symbol* $\\frac{d^nf}{dx^n}$ *for this purpose.*\n\n**Derivative Rules:**\n\n*The constant rule*---i.e., constants can move into and out of derivatives.\n\n$$\n\\frac{d}{dt} \\bigg[c f(t) \\bigg] = c \\frac{d}{dt} \\bigg[f(t)\\bigg] = c \\cdot f^\\prime (t)\n$$ {#eq-constant-rule}\n\n*The sum rule---*i.e., the derivative of the sum is the sum of the derivatives.\n\n$$\n\\frac{d}{dt}\\bigg[ f(t) + g(t) \\bigg] = f^\\prime (t) + g^\\prime (t)\n$$ {#eq-sum-rule}\n\n*The product rule.*\n\n$$\n\\frac{d}{dx} \\bigg[f(x) g(x) \\bigg] = f(x) g^\\prime (x) + g(x) f^\\prime(x)\n$$ {#eq-product-rule}\n\n*The power rule.*\n\n$$\n\\frac{d}{dx} \\bigg[ x^n\\bigg] = n \\cdot x^{n-1}\n$$ {#eq-power-rule}\n\n*Note. All these rules make it possible to estimate the derivative of any polynomial of the form* $c_nx^n + c_{n-1} x^{n-1} + \\dots + c_1$*.*\n\n*The chain rule* allows us to find derivatives of compositions.\n\n$$\n\\frac{d}{dx} \\bigg[ (f \\circ g)(x) \\bigg] = f^\\prime (g(x)) \\cdot g^\\prime (x)\n$$ {#eq-chain-rule}\n\n<aside>$$\n(f \\circ g)(x) = f(g(x))\n$$</aside>\n\nWe can use @eq-power-rule and @eq-chain-rule to figure out the following:\n\n$$\n\\frac{d}{dx} \\bigg[ \\frac{1}{g(x)} \\bigg] = - \\frac{g^\\prime (x)}{g(x)^2}\n$$\n\nUsing this with @eq-product-rule, we can figure out the *quotient rule:*\n\n$$\n\\frac{d}{dx} \\bigg[ \\frac{f(x)}{g(x)}\\bigg] = f^\\prime (x) \\Bigg( \\frac{1}{g(x)} \\Bigg) + f(x) \\Bigg(- \\frac{g^\\prime (x)}{g(x)^2} \\Bigg) = \\frac{f^\\prime (x) g(x) - f(x) g^\\prime (x)}{g(x)^2}\n$$ {#eq-quotient-rule}\n\n## Integrals\n\nWhat is the area $A(a, b)$ below the graph $y = f(x) \\geq 0$, above the $x$-axis, and between $x=a$, and $x=b$? We can estimate $A(a, b)$ with a sum, and then argue that the estimate becomes exact as the number of terms approximates $\\infty$. This is called a *Riemann sum.*\n\n$$\nA(a, b) = \\underbrace{\\int_a^b \\overbrace{f(x)}^{\\small \\text{integrand}} dx}_\\text{integral}\n$$\n\nThe elongated \"s\" stands for \"sum,\" when the number of terms to be summed approaches infinity. The decorations on the \"s\" are called the *lower* and *upper limits of integration*.\n\nThe symbols $f(x) \\cdot dx$ make it clear that the terms of a Riemann sum are the areas of rectangles:\n\n$$\n\\begin{align}\n\\text{base: } (x_{n+1} - x_n), &&\\text{height: }f \\bigg( \\frac{x_n + x_{n+1}}{2} \\bigg)\n\\end{align}\n$$\n\nFor example,\n\n$$\n\\int_0^1 x^2 dx = \\frac{x^3}{3} \\Bigg|_0^1 = \\frac{1^3}{3} - \\frac{0^3}{3} = \\frac{1}{3}\n$$\n\nrepresents the area shaded in pink in the following graph:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + \n  xlim(-0.5, 1.5) + \n  geom_function(fun = \\(x) x^2) +\n  stat_function(geom = \"area\", fun = \\(x) x^2, xlim = c(0, 1), fill = \"pink\") + \n  labs(x = \"x\")\n```\n\n::: {.cell-output-display}\n![](calculus_files/figure-html/unnamed-chunk-2-1.png){width=480}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nintegrate(\\(x) x^2, lower = 0, upper = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.3333333 with absolute error < 3.7e-15\n```\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nn <- 100\nx <- seq(0, 1, length.out = n)\n\nresult <- 0\nfor (i in 1:(n - 1)) {\n  result <- result + (x[i + 1] - x[i]) * ((x[i] + x[i + 1])/2)^2\n}\n\nresult\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3333248\n```\n:::\n:::\n\n\n<aside>100 is much smaller than $\\infty$</aside>\n\nWe use the area under the curve in this graph to think about many things---e.g., displacement of an object with $x$ as time and $y$ as velocity; probability distributions of arbitrary random variables with $y$ as probability densities; etc.\n\n**The Fundamental Theorem of Calculus**\n\n$$\nf(b) - f(a) = \\int_a^b f^\\prime (x) dx\n$$ {#eq-fundamental-theorem}\n\nWe have already seen an example of this when looking for shaded area under $f(x) = x^2$ between $0$ and $1$ in the previous example:\n\n$$\n\\int_0^1 x^2 dx = \\int_0^1 \\frac{d}{dx} \\bigg[ \\frac{1}{3} x^3\\bigg] dx = \\bigg(\\frac{1}{3}x^3\\bigg) \\Bigg|_0^1 = \\frac{1}{3} (1^3) - \\frac{1}{3} (0^3) = \\frac{1}{3}\n$$\n\n<aside>$f|_a^b$ is shorthand for $f(b) - f(a)$</aside>\n\n**Anti-derivatives**\n\nIf $F^\\prime (x) = f(x)$, then the definite integral of $f$ over $[a,b]$ is\n\n$$\n\\int_a^b f(x) dx = F(b) - F(a)\n$$\n\nThe function $F(x)$ is called *an* anti-derivative of $f(x)$.\n\nAll anti-derivatives of $f(x)$ are given by $F(x) + C$ for any *constant of integration* $C$.\n\n*The Power Rule*\n\n$$\n\\begin{align}\n\\int x^n dx = \\frac{1}{n+1} x^{n+1} + C, &&\\text{for } n \\neq -1\n\\end{align}\n$$\n\nand for $n=1$ we have\n\n$$\n\\int x^{-1} dx = \\ln |x| + C\n$$\n\n*Note. We use \"*$\\int$\" for anti-derivatives because they are closely related to integrals thanks to the fundamental theorem of calculus shown in @eq-fundamental-theorem.\n\n*The Sum Rule*\n\n$$\n\\int \\bigg[ f(x) + g(x) \\bigg] dx = \\int f(x) dx + \\int g(x) dx\n$$\n\n*The Constant Rule*\n\n$$\n\\int \\bigg[ c f(x) \\bigg] dx = c \\int f(x) dx\n$$\n\n## Infinite Sums\n\n*Preliminary remarks*\n\nA **sequence** $A_n$ is an ordered list of numbers where the *index* $n$ is drawn from a fixed subset of the integers. In general, the sequence $A_n$ has a limit $L$ if we can make $|A_n - L|$ as small as we like by picking $n$ large enough.\n\n$$\n\\lim_{n \\to \\infty} A_n= L\n$$\n\nExample: *logistic sequence.*\n\nSuppose a population $p_n$ that increases in size over time relative to some carrying capacity (thus, $p$ is a proportion).\n\n$$\np_{n+1} = c \\times p_n \\times (1 - p_n)\n$$\n\n<aside>recursive equation</aside>\n\nHere, $c$ is a positive constant. If the population growth given by this formula eventually stabilizes, we can think of the *long-term population* as $\\lim_{n \\to \\infty} p_n$, assuming that this limit actually exists.\n\n$$\nL = \\frac{c-1}{c}\n$$\n\n<aside>Plug in $L$ for both $p_n$ and $p_{n+1}$ and solve for $L$.</aside>\n\n*However, we don't know if the* $\\lim_{n \\to \\infty} p_n$ *actually exists for all values of* $c$ *and* $p_o$ *(it doesn't).*\n\n**Infinite sums**\n\nAn infinite sum is the limit of the sequence of partial sums:\n\n$$\n\\sum_{j = 1}^\\infty a_j = \\lim_{n \\to \\infty} \\sum_{j = 1}^n a_j\n$$ {#eq-infinite-sum}\n\nWhen this limit exists, we say that the sum **converges**; otherwise we say that it **diverges.**\n\nExample: *the geometric sum*\n\n$$\n\\sum_{j=0}^\\infty r^j\n$$\n\nGeometric sums have a special place in calculus for a variety of reasons, the most important being that we know precisely when they converge and what they converge to.\n\nIt's useful to think of the following sequences:\n\n$$\n\\begin{align}\nS_n &= 1 + r + r^2 + \\dots + r^n, \\\\\nS_{n+1} &= 1 + r + r^2 + \\dots r^{n+1}\n\\end{align}\n$$\n\nFrom which we can establish that:\n\n$$\n\\begin{align}\nS_{n+1} &= r S_n + 1, \\\\\nS_{n+1} &= S_n + r^{n+1}\n\\end{align}\n$$\n\n<aside>Multiply $S_n$ by $r$ and add $1$ to get the first equation.</aside>\n\nAs it turns out, the geometric sum converges if $|r| < 1$ and diverges if $|r| \\geq 1$.\n\nAssuming that the limit exists, and thus $|r| < 1$, we can get the following:\n\n$$\n\\lim_{j \\to \\infty} \\sum_{j=0}^j r^j = \\frac{1}{1-r}\n$$\n\n<aside>Remember to plug in $L$ for both $S_{n+1}$ and $S_n$.</aside>\n\n## Other\n\n### Euler's number\n\nEuler's number $e$ is defined in terms of a limit approaching infinity.\n\n$$\ne = \\lim_{n \\to \\infty} \\bigg( 1 + \\frac{1}{n} \\bigg)^n \\approx 2.718282...\n$$\n\nWe also call this a *horizontal asymptote.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfoo <- function(x) (1 + 1/x)^x\n\nggplot() + \n  xlim(-5, 50) +\n  coord_cartesian(ylim = c(0.5, 5)) +\n  labs(x = \"n\", y = \"f(n)\") +\n  geom_function(fun = foo,  n = 1e3) + \n  geom_hline(yintercept = exp(1), linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](calculus_files/figure-html/unnamed-chunk-4-1.png){width=480}\n:::\n:::\n\n\nNote that the limit is undefined as $n \\to 0$.\n\n**Exponential functions**\n\n> Who has not been amazed to learn that the function $y=e^x$, like a phoenix rising again from its own ashes, is its own derivative?\n\nAn exponential function has the form $f(x) = b^x$ where $b>0$ is constant called the **base** and $x$ sits in the **exponent**.\n\nThe base $e=2.71828 \\dots$ is called **Euler's number**.\n\nAll exponential functions are proportional to their own derivatives, but $e$ alone is the special number for which this proportionality constant is $1$.\n\nFor example, take the following function:\n\n$$\n\\begin{align}\nM(t) = 2^t, && \\lim_{h \\to 0} M = \\frac{\\overbrace{2^t 2^{h}}^{2^{t + h}} - 2^t}{h} = 2^t \\bigg( \\frac{2^{h}-1}{h} \\bigg) \n\\end{align}\n$$\n\nHere, the value $(2^h - 1) / h$ is a constant approximately equal to $0.6931472$.\n\nThus, by definition, we have the following equation:\n\n$$\n\\lim_{h \\to 0} \\frac{e^h - 1}{h} = 1\n$$\n\nAnd so, the derivative of $e^x$ turns out to be $e^x$ itself.\n\n$$\n\\frac{d}{dx}e^x = e^x \\cdot 1\n$$\n\nWith this, we can use the chain rule to get the derivative of the more general form:\n\n$$\n\\frac{d}{dt}e^{ct} = c \\cdot e^{ct}\n$$\n\nFinally, we now have a way to express any proportionality constant in terms of generic natural logarithms (logarithms with base $e$), once we remember that $\\log_e (x) = c$, so that $x = e^c$.\n\n$$\n\\frac{d}{dt} 2^t = \\frac{d}{dt} e^{\\ln(2)t} = \\ln(2) e^{\\ln(2)t}\n$$\n\nThe derivative of $2^t$ is itself multiplied by some proportionality constant equal to $\\ln(2)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog(2, base = exp(1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6931472\n```\n:::\n:::\n\n\n### Taylor Series\n\nA smooth function $f$ can be approximated near any given point $x=a$ by\n\n$$\ny = \\underbrace{\\overbrace{f(a) + f^{(1)} (a) (x-a)}^\\text{linear approximation} + \\frac{1}{2!}f^{(2)}(a)(x-a)^2}_{\\text{quadratic approximation}} + \\frac{1}{3!}f^{(3)}(a)(x-a)^3 + \\dots\n$$\n\nIf we keep going, we end up with a **Taylor series** of $f$ centered at $a$:\n\n$$\n\\sum_{n=0}^\\infty \\frac{f^{(n)}(a)}{n!} (x - a)^n\n$$\n\nFor example,the Taylor series of $e^x$ centered at $0$ is given by:\n\n$$\ne^x = \\sum_{n = 0}^\\infty \\frac{x^n}{n!}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp_taylor_zero <- function(x, degree) {\n  \n  f <- purrr::map(0:degree, function(n) {\n    function(x) x^n / factorial(n)\n  })\n  \n  purrr::map_dbl(x, function(x) {\n    out <- vector(\"double\", length(f))\n    for (i in seq_along(out)) out[[i]] <- f[[i]](x)\n    return(sum(out))\n  })\n}\n\nggplot() + \n  xlim(-2, 4) + \n  geom_function(fun = exp, linewidth = 2) + \n  geom_function(fun = \\(x) exp_taylor_zero(x, degree = 1), aes(color = \"1\")) +\n  geom_function(fun = \\(x) exp_taylor_zero(x, degree = 2), aes(color = \"2\")) +\n  geom_function(fun = \\(x) exp_taylor_zero(x, degree = 3), aes(color = \"3\")) +\n  geom_function(fun = \\(x) exp_taylor_zero(x, degree = 9), aes(color = \"9\")) + \n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  labs(x = \"x\", y = expression(e^x), color = \"degree\")\n```\n\n::: {.cell-output-display}\n![](calculus_files/figure-html/unnamed-chunk-6-1.png){width=480}\n:::\n:::\n\n\n### Limits\n\n$$\n\\lim_{x \\to a} f(x) = L\n$$ {#eq-limit}\n\n*How do we know if this limit exists?*\n\nIf given $\\epsilon > 0$ we can find $\\delta > 0$ so that $0 < |x - a| < \\delta$, this guarantees that $|f(x) - L| < \\epsilon$. Thus, $f$ approaches $L$ as $x$ approaches $a$ and we write @eq-limit.\n\nThis definition of a limit is absolutely necessary for making calculus rigorous.\n\n**Some Limit Theorems**\n\n*The constant rule.*\n\n$$\n\\lim_{x \\to a} \\bigg[ c f(x) \\bigg] = c \\lim_{x \\to a} \\bigg[ f(x) \\bigg]\n$$\n\n*The sum rule.*\n\n$$\n\\lim_{x\\to a} \\bigg[ f(x) + g(x) \\bigg] = \\lim_{x\\to a} \\bigg[ f(x) \\bigg] + \\lim_{x\\to a} \\bigg[ g(x)\\bigg]\n$$\n\n*The power rule.*\n\n$$\n\\lim_{x \\to a} x^n = a^n\n$$\n",
    "supporting": [
      "calculus_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}