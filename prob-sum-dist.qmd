# Summarizing Distributions

```{r}
#| include: false

library(tidyverse)
theme_set(theme_light(base_family = "Crimson Text"))
```

## Expectation

The expected value (also known as the *expectation* or *mean*) is the most common measure of the "center" of a probability distribution.

-   Discrete random variables

$$
E[X] = \sum_{\textsf{supp}(x)} x f(x)
$$

-   Continuous random variables

$$
E[X] = \int_{-\infty}^\infty x f(x)dx
$$

-   **Properties of expected values**

$$
\begin{align}
&E[c] = c &\text{ for all } c \in \mathbb R \\\\
&E[cX] = c\ E[X]  &\text{ for all } c \in \mathbb R
\end{align}
$$

-   Expectation of a Bernoulli random variable

$$
E[X] = P(X = 1) = p
$$

**Expectation of a function of a random variable** (also known as *LOTUS*). This comes up in many applications (e.g. finding the *variance* of a random variable), but the result is far from obvious (see [here](#lotus)).

-   Continuous case

$$E[g(X)] = \int_{-\infty}^\infty g(x) f_X(x) dx$$

-   Discrete case

$$E[g(X)] = \sum_{\textsf{supp}(x)} g(x) f_X(x)$$

-   **Linearity of expectations**

$$E[aX + bY + c] = aE[X] + bE[Y] + c$$

Note that this property follows from considering the expectation of a function of a bivariate joint distribution.

$$\underbrace{g(X, Y) = aX + bY + c}_\text{function of a bivariate joint distribution}$$

Apply *LOTUS* and *marginalization*:

$$
\begin{align}
E[g(X, Y)] &= \sum_x \sum_y g(X, Y) f_{XY}(x,y) \\\\ &= 
\sum_x \sum_y (ax + by + c) f_{XY}(x,y) \\\\ &=
a\sum_x \sum_y x f_{XY}(x,y) +  b\sum_x \sum_y y f_{XY}(x,y)+ c\sum_x \sum_y f_{XY}(x,y)
\end{align}
$$

## Moments and variances

We can generalize expectations to further characterize the features of a distribution. This is the idea of **raw moments**, among which the expected value is just a special case.

$$
\underbrace{\mu_j^\prime = E[X^j]}_{j^{th}\text{ raw moment }}
$$

Raw moments provide summary information about a distribution, describing elements of its *shape* and *location*.

-   **Central moments**

$$\mu_j = E\left[(X - E[X])^j\right]$$

The sole distinction between raw and central moments lies in whether or not the expected value of $X$ is subtracted before calculations.

-   The **variance** (second central moment)

$$
V[X] = E\left[(X - E[X])^2\right] = E\left[X^2\right] - E[X]^2
$$

The variance measures the expected value of the squared difference between the observed value of $X$ and its mean. Note that the first central moment equals zero.

-   **Properties of variances**

$$
\begin{align}
&V[X + c] = V[X] &\text{ for all } c \in \mathbb R \\\\
&V[cX] = c^2\ E[X]  &\text{ for all } c \in \mathbb R
\end{align}
$$

-   **Standard deviation**

$$
\sigma[X] = \sqrt{V[X]}
$$

The standard deviation is often preferable to the variance, since it is on the same scale as the random variable of interest.

Knowing these two quantities ($E[X]$ and $\sigma[X]$) tells *everything* about normal distributions.

-   **The normal distribution**

$$X \sim \textsf{normal}(\mu, \sigma)$$

$$
E[X] = \mu \hspace{0.5cm} \text{ and } \hspace{0.5cm}
\sigma[X] = \sigma
$$

$$
f_X(x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right)
$$

Any linear combination of any number of mutually independent normal random variables must itself be normal.

$$
X \perp Y \to X + Y \sim \textsf{normal}\left(\mu_X + \mu_Y, \sqrt{\sigma_X^2 + \sigma_Y^2}\right)
$$

## Mean Squared Error

MSE is a metric that characterizes how well a random variable $X$ approximates a certain value $c$.

$$MSE = E\left[(X - c)^2\right]$$

Note that the MSE about zero is the same as the *second raw moment*, and the MSE about $E[X]$ is the same as the *second central moment*, which is also the variance.

-   **Root Mean Squared Error**

    $$\sqrt{E\left[(X - c)^2\right]}$$

    Note. This is used as a common measure of accuracy in the context of estimation.

-   **Decomposition**

$$
\begin{align}
E\left[(X - c)^2\right] &= E\left[(X^2 - 2cX + c^2\right] \\\\ &=
E\left[X^2\right] - 2cE[X] + c^2 \\\\ &=
E\left[X^2\right] \underbrace{- E[X]^2 + E[X]^2}_\text{clever trick} - 2cE[X] + c^2 \\\\ &= 
\left(E\left[X^2\right] - E[X]^2\right) + \left(E[X]^2 - 2cE[X] + c^2\right) \\\\ &=
V[X] + (E[X]-c)^2
\end{align}
$$

**Note.** In the context of estimation, this is also known as the *bias-variance decomposition*.

The MSE is also linked to an alternative definition of the mean: the value $c$ that minimizes the MSE of $X$ is $E[X]$.

$$
\underset{c \in \mathbb R}{\arg \min}\ E\left[(X - c)^2\right] = E[X]
$$

If we where to choose a different "loss function" besides the MSE, we could come up with different "best" choices for $c$. For example, the *median* is the value $c$ that minimizes the Mean Absolute Error ($|X - c|$).

# Summarizing joint distributions

## Covariance

Covariance measures the extent to which two random variables "move together."

$$
\begin{align}
\text{Cov}[X, Y] &= E\big[(X - E[X])(Y - E[Y])\big] \\\\ &=
E[XY] - E[X]E[Y]
\end{align}
$$

-   **Variance Rule** (non-linearity of variances)

$$V[X + Y] = V[X] + 2\text{Cov}[X, Y] + V[Y]$$

-   **Variance is a special case of Covariance**

$$\text{Cov}[X, X] = V[X]$$

-   **Covariance of sums**

$$
\text{Cov}[X + W, Y + Z] = \text{Cov}[X, Y] + \text{Cov}[X, Z] + \text{Cov}[W, Y] + \text{Cov}[W, Z]
$$

Much like standard deviation rescales variance, correlation rescales covariance to make its interpretation clearer. The **correlation** of two random variables is as follows:

$$
\rho[X, Y] = \frac{\text{Cov}[X, Y]}{\sigma[X] \sigma[Y]}
$$

The correlation $\rho$ is bounded in $[-1, 1]$, a fact that derives from the *Cauchy-Schwarz inequality*.

**Linear dependence** describes the relationship between two random variables where one can be written as a linear function of the other. And correlation measures the degree of linear dependence between two random variables.

$$
\begin{align}
&\rho[X, Y] = 1 \iff Y = a + bX \\\\
&\rho[X, Y] = -1 \iff Y = a - bX \\\\
&\text{where } b > 0 \text{ and } a,b \in \mathbb R
\end{align}
$$

If two random variables are *linearly* independent, then $\text{Cov}[X, Y] = 0$. This fact follows from the definition of covariance and the application of LOTUS.

$$
\begin{align}
E[XY] &= \int_{-\infty}^\infty \int_{-\infty}^\infty x y f(x, y)dydx \\\\ &=
\int_{-\infty}^\infty \int_{-\infty}^\infty xy f_X(x)f_Y(y)dydx \\\\ &=
\int_{-\infty}^\infty x f_X(x)dx \int_{-\infty}^\infty y f_Y(y) \\\\ &=
E[X]E[Y]
\end{align}
$$

That is, *no relationship between two random variables implies no linear relationship them.* However, the opposite is not true: lack of correlation does *not* imply independence.

$$
X \perp Y \longrightarrow \text{Cov}[X, Y] = 0 
$$

## Conditional Expectations

Conditional expectations allow us to describe how the "center" of one random variable's distribution changes once we condition on the observed value of another random variable.

-   Discrete case ($\forall x \in \textsf{supp}[X]$)

$$
E[Y \mid X = x] = \sum_y y f_{Y \mid X}(y \mid x)
$$

-   Continuous case ($\forall x \in \textsf{supp}[X]$)

$$
E[Y \mid X = x] = \int_{-\infty}^\infty y f_{Y \mid X}(y \mid x)dy
$$

Note that LOTUS can also be applied to conditional expectations of functions of many random variables.

$$
E[g(X, Y) \mid X = x] = \int_{-\infty}^\infty g(x, y) f_{Y \mid X}(y \mid x)dy
$$

Unlike unconditional expectations, $E[Y \mid X = x]$ is a *family of operators* on the random vector $(X, Y)$ that is indexed by $x$.

A **conditional expectation function** (CEF) is just a conditional expectation takes is sometimes denoted by $G_Y(x)$ to emphasize the fact that it's a *function* that maps $x$ to $E[Y \mid X = x]$ rather than the value of $E[Y \mid X = x]$ at some particular $x$. This notation also emphasizes the fact that it's a function of $x$, not a function of the random variable $Y$.

We write $E[X \mid Y]$ to denote $G_Y(X)$ which is a function of the random variable of $X$, and thus it's also a random variable.

The CEF is closely linked to various topics such as *regression*, *missing data*, and *causal inference*.

-   **Law of total expectations** (also known as the law of iterated expectations or Adam's law).

    $$E[Y] = E\big[E[Y \mid X]\big]$$

    It implies that the unconditional expectation can be represented as a weighted average of conditional expectations, where the weights are proportional to the probability distribution of the variable being conditioned on.

    *Proof*

$$
\begin{align}
E[Y] &= \sum_y y f_Y(y) \\\\ &=
\sum_y y \sum_x f_{X, Y}(x, y) \\\\ &= 
\sum_x \sum_y y f_{X \mid Y}(y \mid x) f_X(x) \\\\ &=
\underbrace{\sum_x \overbrace{\bigg(\sum_y y f_{Y \mid X} (y \mid x)\bigg)}^{E[Y \mid X = x]} f_X (x)}_{E\big[E[Y \mid X]\big]}
\end{align}
$$

-   **Law of total variance** (also known as Eve's law).

    $$V[Y] = \underbrace{E\big[V[Y \mid X]\big]}_\text{within-group variation} + \underbrace{V\big[E[Y \mid X]\big]}_\text{between-group variation}$$

    This theorem allows us to decompose the variability of a random variable $Y$ into the average variability "within" values of $X$ and the variability "across" values of $X$.

    The ordering of the $E$'s and $V$'s spells out $EVVE$, hence the name "Eve's law".

    *Proof*.

    1.  $E[Y] = E\big[E[Y \mid X]\big] = E[G_Y(X)]$ (Adam's law)

    2.  $E \big[V[Y \mid X]\big] = E\big[E[Y^2 \mid X] - G_Y(X)^2\big] = E\big[Y^2\big] - E\big[G_Y(X)^2\big]$

    3.  $V[E[Y \mid X]] = E\big[G_Y(X)^2\big] - E[G_Y(X)]^2 = E\big[G_Y(X)^2\big] - E[Y]^2$

    We add (2) and (3) to get back to the original definition of $V[Y] = E\big[Y^2\big] - E[Y]^2$.

We can also look at Eve's law from a different perspective:

-   "Another way to think about Eve's law is in terms of *prediction*. If we wanted to predict someone's height ($Y$) based on their age ($X$) alone, the ideal scenario would be if everyone within an age group had exactly the same height, while different age groups had different heights. Then, given someone's age, we would be able to predict their height perfectly. In other words, the ideal scenario for prediction is *no* within-group variation in height, since the within-group variation cannot be explained by age differences. For this reason, within-group variation is also called *unexplained variation*, and between-group variation is also called *explained variation*. Eve's law says that the total variance of $Y$ is the sum of unexplained and explained variation" (Blitzstein & Hwang 2014).

## Best Predictors

Suppose we knew the full joint cumulative distribution function (CDF) of $X$ and $Y$, and then someone gave us a randomly drawn value of $X$. What would be the guess of $Y$ that would have the lowest MSE? The function $g(X)$ that best approximates $Y$ is the CEF.

-   $E[Y|X]$, is the best (minimum MSE) predictor of $Y$ given $X$.

This makes the CEF a natural target of inquiry: if the CEF is known, much is known about how $X$ relates to $Y$. But in many cases the CEF can turn out to be an extremely complicated function.

What if we were to restrict ourselves to just *linear functions* of the form $g_Y(x) = a + b X$? We could then define the **best linear predictor** (BLP) of $Y$ given $X$ as the set of values $(a, b)$ that minimizes the MSE.

The BLP is expressed as:

$$
(\alpha, \beta) = \underset{a, b \in \mathbb R}{\arg \min}\ E\big[(Y - a + b X)^2\big]
$$

Using some calculus, one can show that these values are:

$$
\begin{align}
&\alpha = E[Y] - \frac{\text{Cov}[X, Y]}{V[X]} E[X] \\\\ 
&\beta =  \frac{\text{Cov}[X, Y]}{V[X]}
\end{align}
$$

**Note.** This expression is identical to the one provided by the method of *ordinary least squares* (OLS), which is designed to estimate population parameters from sample data.

There are two important corollaries that follow from this:

1.  The BLP is also the best linear approximation of the CEF.

$$
(\alpha, \beta) = \underset{a, b \in \mathbb R}{\arg \min}\ E\bigg[\big(E[Y \mid X] - (a + b X)\big)^2\bigg]
$$

2.  If the CEF is linear, then the CEF *is* the BLP.

Note that these are some of the implications of **independence** for conditional expectations:

$$
\begin{align}
Y \perp X \iff &1.\ E[Y \mid X] = E[Y] \\\\ &2. V[Y \mid X] = V[Y] \\\\ &3. \text{ The BLP of } Y \text{ given } X \text{ is } E[Y] 
\end{align}
$$

**Plotting the CEF and BLP**

Suppose we have the following random variables and that we are interested in approximating the CEF $G_Y(x)$ with it's BLP.

$$
\begin{align}
&X \sim \text{uniform}(0, 1) \\\\
&W \sim \text{normal}(0, 1) \\\\
&Y = 10X^2 + W
\end{align}
$$

By linearity of expectations, can conclude that the CEF is just

$$
E[Y \mid X] =  10 X^2
$$

And that the BLP (after some algebra) is given by

$$
\begin{align}
&\beta = \frac{10 \big(E\big[X^3\big] - E[X]E\big[X^2\big]\big)}{E\big[X^2\big] - E[X]^2} \\\\
&\alpha = E[10X^2 + W] - \beta E[X]
\end{align}
$$

The final solution is given by integrating over $X \sim \text{uniform}(0, 1)$:

$$
(\alpha, \beta) = \left(-\frac{5}{3}, 10\right)
$$

And plotting both functions over $\textsf{supp}[X]$ looks like this:

```{r}
#| code-fold: true

E <- function(m) {
  output <- integrate(
    f = function(x) x^m * fX(x), 
    lower = -Inf, upper = Inf
    )$value
}

slope <- function() {
  num <- 10 * (E(3) - E(1) * E(2))
  denom <- E(2) - E(1)^2
  return(num / denom)
}

intercept <- function() {
  10 * E(2) - slope() * E(1)
}

CEF <- function(x) 10*x^2
fX <- function(x) dunif(x, min = 0, max = 1)

tibble(x = c(-1, 2)) %>% 
  ggplot(aes(x)) +
  geom_function(fun = CEF, color = "steelblue") +
  geom_function(fun = function(x) intercept() + slope()*x, 
                color = "tomato") + 
  geom_rect(aes(xmin = 0, xmax = 1, ymin = -Inf, ymax = Inf), 
            alpha = 1/10)
```

Here, the BLP (in red) approximates the CEF (in blue) reasonably well over the domain of $X$. While this is not always the case, it is very often the case in the social and health sciences. The BLP is thus a good "first approximation" in a very literal sense, in that it is an approximation with a first-order polynomial.

However, when the CEF is not linear, one must take care in interpreting the BLP as equivalent to the CEF. In particular, this may pose a problem when attempting to make inferences where $X$ has *low probability mass* over some parts of the domain of the CEF. This is because *under nonlinearity, the BLP depends on the distribution of* $X$.

<aside>See [**here**](https://acastroaraujo.shinyapps.io/CEF-BLP/) for an interactive demonstration.</aside>

## Properties of residuals

We define deviations (or residuals) with the letter $\epsilon$ and note that they have similar properties when considering with respect to either the CEF or the BLP.

**Properties of deviations**

```{r, echo=FALSE}
CEF <- c("$\\epsilon = Y - E[Y \\mid X]$", "$E[\\epsilon] = 0$",
         "$E[\\epsilon \\mid X] = 0$", "$\\text{Cov}[\\epsilon, g(X)] = 0$",
         "$V[\\epsilon \\mid X] = V[Y \\mid X]$", 
         "$V[\\epsilon] = E\\big[V[Y \\mid X]\\big]$")

BLP <- c("$\\epsilon = Y - (\\alpha + \\beta X)$", "$E[\\epsilon] = 0$", 
         "$E[\\epsilon X] = 0$ (independence)", "$\\text{Cov}[\\epsilon, X] = 0$",
         "", "")

knitr::kable(data.frame(CEF, BLP), booktabs = TRUE, align = "c",
             caption = "**Properties of deviations**")
```
