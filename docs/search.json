[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Building Blocks",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\nDifferential Equations\nLinear Algebra\nProbability"
  },
  {
    "objectID": "prob.html",
    "href": "prob.html",
    "title": "Probability",
    "section": "",
    "text": "The difference between probability and statistical inference is succinctly described by Wasserman (2004):\n\nProbability\n\nGiven a data generating process, what are the properties of the outcomes?\n\nStatistical Inference\n\nGiven the outcomes, what can we say about the process that generated the data?\n\n\nThere are at least three common interpretations of probability.\nThe classical interpretation is based on the idea of equally likely outcomes—i.e., if the outcome of some process must be \\(m\\) of \\(n\\) different outcomes, and these are equally likely to occur, then the probability of \\(m\\) is \\(m/n\\). This interpretation is silly because the idea of “equally likely outcome” is based on the concept of probability that we are trying to define, it’s a tautology.\n\nMost interesting outcomes cannot be assumed to be equally likely!\n\nAn extension of this interpretation leads to an interpretation of probability as a calculus for counting that allows us to represent the plausibility of different events. This is also known as the naive definition of probability. It is very limited because it requires equally likely outcomes and can’t handle uncountable or infinite sample spaces (e.g., areas, continuous time measurements). Better interpretations of probability build upon the same mathematical foundations.\nAccording to the frequentist interpretation, the probability that some specific outcome of a process will be obtained can be interpreted to mean the relative frequency with which that outcome would be obtained if the process were repeated a large number of times and under similar conditions. In other words, probability describes the proportions of specific events that can be expected to occur among many realizations of a random generative process. It's all about long-term frequencies. Probabilities are the long-term frequency properties of things that we can measure repeatedly. This interpretation is not silly, but it only applies to problems in which there can be (at least in principle) a large number of similar repetitions of a certain process. Many important problems are not of this kind.\n\nWhat number of times is large enough?\nThe conditions must not be \"exactly\" the same; otherwise the outcomes would be exactly the same!\n\nThe Bayesian interpretation takes a different direction. Here, we treat “randomness” as a property of information, not of or the world. Almost nothing in the world is actually random. Presumably, we could predict everything exactly if we had more information. Thus, we use randomness to describe uncertainty in the face of incomplete knowledge. This is why this is also known as the subjective interpretation of probability.\n\nIt will make some readers uncomfortable to suggest that there is more than one way to define “probability.” Aren't mathematical concepts uniquely correct? They are not. Once you adopt some set of premises, or axioms, everything does follow logically in mathematical systems. But the axioms are open to debate and interpretation. So not only is there \"Bayesian\" and \"frequentist\" probability, but there are different versions of Bayesian probability even, relying upon different arguments to justify the approach.\nMcElreath (2020, 12–13)\n\nRegardless of the ongoing controversy about its interpretation, probability was established as a mathematical theory by Kolmogorov in the early 20th century. And although the different interpretations seem incompatible, the calculus of probability applies equally well no matter which interpretation one prefers. Thus, probability is simply a mathematical tool that allows us allows us to quantify uncertainty and randomness in principled ways.\nAxioms\nfile:///Users/acastroaraujo/Documents/Notes/Math/Probability.html\nfile:///Users/acastroaraujo/Documents/Columbia/3.%20Fall/BDA%20-%20Gelman/Notes_-_BDA.html\nfile:///Users/acastroaraujo/Documents/Columbia/1.%20Fall/Probability%20Theory/Other_Stuff.html\nfile:///Users/acastroaraujo/Documents/Columbia/1.%20Fall/Probability%20Theory/Probability.html\nfile:///Users/acastroaraujo/Documents/Old%20Repositories/SelfStudy/Probability-and-Statistics/1-Probability.html\nhttps://acastroaraujo-notebooks.netlify.app/posts/2021-01-27-probability-notes/\nhttps://betanalpha.github.io/assets/case_studies/probability_theory.html#12_sigma_algebras\nAdd separate entry for:\nhttps://acastroaraujo-notebooks.netlify.app/posts/2021-01-31-counting-methods/\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. CRC press.\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in Statistical Inference. Vol. 26. Springer."
  },
  {
    "objectID": "prob-counting.html#the-sampling-table",
    "href": "prob-counting.html#the-sampling-table",
    "title": "1  Counting Methods",
    "section": "1.1 The Sampling Table",
    "text": "1.1 The Sampling Table\nThe sampling table gives the number of possible samples of size \\(k\\) out of a population of size \\(n\\), under various assumptions about how the sample is collected.\n\n\n\n\n \n  \n      \n    Ordered \n    Unordered \n  \n \n\n  \n    With replacement \n    $n^k$ \n    $\\binom{n + k - 1}{k}$ \n  \n  \n    Without replacement \n    $\\frac{n}{(n-k)!}$ \n    $\\binom{n}{k}$ \n  \n\n\n\n\n\n\n\n\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to Probability. Crc Press."
  },
  {
    "objectID": "rn.html",
    "href": "rn.html",
    "title": "Generating Random Numbers",
    "section": "",
    "text": "How do we simulate random numbers from specific probability distributions?\nHow do we use random numbers to do stuff?\n\nThe simplest case involves generating uniform pseudo-random numbers. Methods for generating random numbers from other probability distributions all depend on the uniform random number generator.\nThis set of notes builds upon three black boxes: runif(), sample(), and .Random.seed. It’s no use pretending I know how any of this stuff work.\nADD SOME STUFF FROM THE BOOK I LOANED NICO.\n\nrunif()\nADD EXPLANATION OF THIS.\n\ndunif\n\nfunction (x, min = 0, max = 1, log = FALSE) \n.Call(C_dunif, x, min, max, log)\n<bytecode: 0x7fb115d05150>\n<environment: namespace:stats>\n\npunif\n\nfunction (q, min = 0, max = 1, lower.tail = TRUE, log.p = FALSE) \n.Call(C_punif, q, min, max, lower.tail, log.p)\n<bytecode: 0x7fb115da0aa8>\n<environment: namespace:stats>\n\nqunif\n\nfunction (p, min = 0, max = 1, lower.tail = TRUE, log.p = FALSE) \n.Call(C_qunif, p, min, max, lower.tail, log.p)\n<bytecode: 0x7fb115df9468>\n<environment: namespace:stats>\n\nrunif\n\nfunction (n, min = 0, max = 1) \n.Call(C_runif, n, min, max)\n<bytecode: 0x7fb0f52657a8>\n<environment: namespace:stats>\n\n\n\n\nsample()\nR also has a function that allows sampling from finite populations. This sample() function can be used with our without replacement.\n\n## toss six coins\nsample(0:1, size = 6, replace = TRUE)\n\n[1] 0 1 0 0 1 1\n\n## permuation of letters A-Z\nsample(LETTERS) \n\n [1] \"E\" \"J\" \"N\" \"M\" \"Q\" \"F\" \"U\" \"V\" \"H\" \"G\" \"Z\" \"X\" \"A\" \"I\" \"P\" \"S\" \"R\" \"O\" \"C\"\n[20] \"K\" \"Y\" \"W\" \"L\" \"B\" \"T\" \"D\"\n\n## sample from a multinomial distribution\np <- c(0.2, 0.3, 0.5)\nx <- sample(1:3, size = 3000, replace = TRUE, prob = p)\ntable(x)\n\nx\n   1    2    3 \n 608  885 1507 \n\ntable(x) / length(x)\n\nx\n        1         2         3 \n0.2026667 0.2950000 0.5023333 \n\n\n\n\n.Random.seed\nNote. There’s an integer vector called .Random.seed in the global environment for every R session. This vector changes every time you generate random numbers or every time you change seeds.\nFor example:\n\nset.seed(111)\nstr(.Random.seed)\n\n int [1:626] 10403 624 -762750981 -378653952 -1193945343 -1238812466 800256759 999955148 -1466969763 438919290 ...\n\nset.seed(222)\nstr(.Random.seed)\n\n int [1:626] 10403 624 1488669722 -588037421 -1945880072 -1874066535 1822860934 510535503 495156548 -856964235 ...\n\nrunif(1) ## generate one random number\n\n[1] 0.9315925\n\nstr(.Random.seed)\n\n int [1:626] 10403 1 -2033833551 328541280 -1581583874 -1734191735 219878075 -974258550 -1901840020 939491055 ...\n\nset.seed(111)\nstr(.Random.seed)\n\n int [1:626] 10403 624 -762750981 -378653952 -1193945343 -1238812466 800256759 999955148 -1466969763 438919290 ..."
  },
  {
    "objectID": "rn-methods.html#the-inverse-transform-method",
    "href": "rn-methods.html#the-inverse-transform-method",
    "title": "3  Methods",
    "section": "3.1 The Inverse Transform Method",
    "text": "3.1 The Inverse Transform Method"
  },
  {
    "objectID": "rn-methods.html#the-acceptance-rejection-method",
    "href": "rn-methods.html#the-acceptance-rejection-method",
    "title": "3  Methods",
    "section": "3.2 The Acceptance-Rejection Method",
    "text": "3.2 The Acceptance-Rejection Method"
  },
  {
    "objectID": "rn-methods.html#transformations",
    "href": "rn-methods.html#transformations",
    "title": "3  Methods",
    "section": "3.3 Transformations",
    "text": "3.3 Transformations"
  },
  {
    "objectID": "rn-methods.html#sums-and-mixtures",
    "href": "rn-methods.html#sums-and-mixtures",
    "title": "3  Methods",
    "section": "3.4 Sums and Mixtures",
    "text": "3.4 Sums and Mixtures\nhttp://www.columbia.edu/~ks20/4404-Sigman/4404-Notes-ITM.pdf\nhttps://stephens999.github.io/fiveMinuteStats/inverse_transform_sampling.html\nhttps://en.wikipedia.org/wiki/Inverse_transform_sampling"
  },
  {
    "objectID": "rn-mcmc.html",
    "href": "rn-mcmc.html",
    "title": "5  MCMC",
    "section": "",
    "text": "all from the rizzo book"
  },
  {
    "objectID": "cm.html",
    "href": "cm.html",
    "title": "Correlation Matrices",
    "section": "",
    "text": "This section follows the structure of Hadd and Rodgers (2020).\n\nWhat are correlation matrices? Some math here.\nHow to test correlation matrices? Some null hypothesis testing here.\nHow to model correlation matrices? Some PCA, CFA, and SEM here.\nHow to visualize correlation matrices? This section contains some stuff about geometry and correlation space.\n\n\n\n\n\nHadd, Alexandria, and Joseph Lee Rodgers. 2020. Understanding Correlation Matrices. SAGE Publications."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Blitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to\nProbability. Crc Press.\n\n\nHadd, Alexandria, and Joseph Lee Rodgers. 2020. Understanding\nCorrelation Matrices. SAGE Publications.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in r and Stan. CRC press.\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in\nStatistical Inference. Vol. 26. Springer."
  }
]