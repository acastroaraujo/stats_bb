[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Building Blocks",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\nDifferential Equations\nLinear Algebra\nProbability"
  },
  {
    "objectID": "prob.html#interpretations",
    "href": "prob.html#interpretations",
    "title": "Probability",
    "section": "Interpretations",
    "text": "Interpretations\nThere are at least three common interpretations of probability.\nThe classical interpretation is based on the idea of equally likely outcomes—i.e., if the outcome of some process must be \\(m\\) of \\(n\\) different outcomes, and these are equally likely to occur, then the probability of \\(m\\) is \\(m/n\\). This interpretation is silly because the idea of “equally likely outcome” is based on the concept of probability that we are trying to define, it’s a tautology.\n\nMost interesting outcomes cannot be assumed to be equally likely!\n\nAn extension of this interpretation leads to an interpretation of probability as a calculus for counting that allows us to represent the plausibility of different events. This is also known as the naive definition of probability. It is very limited because it requires equally likely outcomes and can’t handle uncountable or infinite sample spaces (e.g., areas, continuous time measurements). Better interpretations of probability build upon the same mathematical foundations.\nAccording to the frequentist interpretation, the probability that some specific outcome of a process will be obtained can be interpreted to mean the relative frequency with which that outcome would be obtained if the process were repeated a large number of times and under similar conditions. In other words, probability describes the proportions of specific events that can be expected to occur among many realizations of a random generative process. It’s all about long-term frequencies. Probabilities are the long-term frequency properties of things that we can measure repeatedly. This interpretation is not silly, but it only applies to problems in which there can be (at least in principle) a large number of similar repetitions of a certain process. Many important problems are not of this kind.\n\nWhat number of times is large enough?\nThe conditions must not be “exactly” the same; otherwise the outcomes would be exactly the same!\n\nThe Bayesian interpretation takes a different direction. Here, we treat “randomness” as a property of information, not of or the world. Almost nothing in the world is actually random. Presumably, we could predict everything exactly if we had more information. Thus, we use randomness to describe uncertainty in the face of incomplete knowledge. This is why this is also known as the subjective interpretation of probability.\n\nIt will make some readers uncomfortable to suggest that there is more than one way to define “probability.” Aren’t mathematical concepts uniquely correct? They are not. Once you adopt some set of premises, or axioms, everything does follow logically in mathematical systems. But the axioms are open to debate and interpretation. So not only is there “Bayesian” and “frequentist” probability, but there are different versions of Bayesian probability even, relying upon different arguments to justify the approach.\nMcElreath (2020, 12–13)\n\nRegardless of the ongoing controversy about its interpretation, probability was established as a mathematical theory by Kolmogorov in the early 20th century. And although the different interpretations seem incompatible, the calculus of probability applies equally well no matter which interpretation one prefers. Thus, probability is simply a mathematical tool that allows us allows us to quantify uncertainty and randomness in principled ways."
  },
  {
    "objectID": "prob.html#extra",
    "href": "prob.html#extra",
    "title": "Probability",
    "section": "Extra",
    "text": "Extra\nfile:///Users/acastroaraujo/Documents/Notes/Math/Probability.html\nfile:///Users/acastroaraujo/Documents/Columbia/3.%20Fall/BDA%20-%20Gelman/Notes_-_BDA.html\nfile:///Users/acastroaraujo/Documents/Columbia/1.%20Fall/Probability%20Theory/Other_Stuff.html\nfile:///Users/acastroaraujo/Documents/Columbia/1.%20Fall/Probability%20Theory/Probability.html\nfile:///Users/acastroaraujo/Documents/Old%20Repositories/SelfStudy/Probability-and-Statistics/1-Probability.html\nhttps://acastroaraujo-notebooks.netlify.app/posts/2021-01-27-probability-notes/\nhttps://betanalpha.github.io/assets/case_studies/probability_theory.html#12_sigma_algebras\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. CRC press.\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in Statistical Inference. Vol. 26. Springer."
  },
  {
    "objectID": "prob-intro.html#kolmogorov-axioms",
    "href": "prob-intro.html#kolmogorov-axioms",
    "title": "1  Introduction",
    "section": "1.1 Kolmogorov Axioms",
    "text": "1.1 Kolmogorov Axioms\n\\((\\Omega, S, P)\\) form a probability space when the following conditions hold:\n\nNon-negativity, for any \\(A \\in S\\)\n\n\\[0 \\leq P(A) \\leq 1\\]\n\nNormalization.\n\n\\[P(\\Omega) = 1\\]\n\nCountable additivity, if \\(A_1, A_2, ... \\in S\\) are pairwise disjoint, then\n\n\\[P(A_1 \\cup A_2, \\cup ...) = P(A_1) + P(A_2) + ... \\]\nWe can represent any random generative process as a probability space \\((\\Omega,S,P)\\).\n\n1.1.1 Consequences\nSeveral other fundamental properties of probability follow directly from the Kolmogorov probability axioms:\nLet \\(A, B \\in S\\)\n\nMonotonicity\n\n\\[A \\subseteq B \\to P(A) \\leq P(B)\\]\n\nSubtraction rule\n\n\\[A \\subseteq B \\to P(B \\setminus A) = P(B) - P(A)\\]\n\nSome event in \\(S\\) must occur\n\n\\[P(\\varnothing) = 0\\]\n\nComplement rule\n\n\\[P(A^C) = 1 - P(A)\\]"
  },
  {
    "objectID": "prob-intro.html#joint-and-conditional-probabilities",
    "href": "prob-intro.html#joint-and-conditional-probabilities",
    "title": "1  Introduction",
    "section": "1.2 Joint and Conditional Probabilities",
    "text": "1.2 Joint and Conditional Probabilities\nThe joint probability of of \\(A\\) and \\(B\\) (i.e. that both events will ocur in a single draw from \\((\\Omega, S, P)\\)) is denoted as \\(P(A \\cap B)\\).\nFor example, the probability of rolling a six-sided die and getting \\(A = \\{\\omega \\in \\Omega: \\omega \\geq 4\\}\\) and \\(B = \\{\\omega \\in \\Omega: \\omega \\text{ is even}\\}\\) is as follows:\n\\[\nP(A \\cap B) = P(\\{4, 5, 6\\} \\cap \\{2, 4, 6\\}) = \\frac{|\\{4, 6\\}|}{|\\Omega|} = \\frac{1}{3}\n\\]\n\nAddition rule\n\n\\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\]\nThe conditional probability of \\(A\\) given \\(B\\) is denoted as\n\\[P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\\]\n\nProduct rule\n\n\\[P(A \\mid B) P(B) = P(A \\cap B)\\]\n\nBayes’ rule\n\n\\[P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}\\]\nThe law of total probability states that if \\(\\{A_1, A_2, A_3, ...\\}\\) is a partition of \\(\\Omega\\) and \\(B \\in S\\), then\n\\[\nP(B) = \\sum_i P(B \\cap A_i) = \\sum_i \\underbrace{P(B \\mid A_i) P(A_i)}_\\text{product rule}\n\\]\nIn other words, the probability of an event \\(B\\) is effectively a weighted average of the conditional probabilities of that event (\\(B\\)).\nEvents and conditional probabilities.\nWhen writing \\(\\Pr(A\\mid E)\\), we do not mean that \\(A \\mid E\\) is an event and that we’re taking its probability.\n\\[A \\mid E \\hspace{0.3cm} \\text{is not an event!}\\]\n\n\\(\\Pr(A \\mid E)\\) is a probability function which assigns probabilities in accordance with the knowledge that \\(E\\) has occurred.\n\\(\\Pr(A)\\) is a different probability function which assigns probabilities without regard for whether \\(E\\) has occurred or not."
  },
  {
    "objectID": "prob-intro.html#independence",
    "href": "prob-intro.html#independence",
    "title": "1  Introduction",
    "section": "1.3 Independence",
    "text": "1.3 Independence\nEvents \\(A, B \\in S\\) are independent if \\(P(A \\cap B) = P(A)P(B)\\). This also implies the following:\n\\[A \\perp B \\iff P(A \\mid B) = P(A)\\]\nThat is, when \\(A\\) and \\(B\\) are independent, knowing whether or not \\(B\\) has occurred gives us no information about the probability that \\(A\\) has occurred. This is a very strong assumption, but it lies in the heart of many applications in statistics that work with independent and identically distributed (i.i.d.) random variables.\n\nConditional independence\n\n\\[A \\perp B \\mid E \\iff P(A \\cap B \\mid E) = P(A \\mid E) P(B \\mid E)\\]\nThis is different from saying that \\(A\\) and \\(B\\) are independent themselves.\nIn general:\n\nTwo events can be conditionally independent given \\(E\\), but not independent.\nTwo events can be independent, but not conditionally independent given \\(E\\).\nTwo events can be conditionally independent given \\(E\\), but not independent given \\(E^C\\)."
  },
  {
    "objectID": "prob-counting.html#sampling",
    "href": "prob-counting.html#sampling",
    "title": "2  Counting Methods",
    "section": "Sampling",
    "text": "Sampling\nTable 2.1 gives the number of possible samples of size \\(k\\) out of a population of size \\(n\\), under various assumptions about how the sample is collected.\n\n\nTable 2.1: Sampling Table\n\n\n\n\n\n\n\n\nOrdered\nUnordered\n\n\nWith replacement\n\\(n^k\\)\n\\(\\binom{n + k - 1}{k}\\)\n\n\nWithout replacement\n\\(\\frac{n}{(n-k)!}\\)\n\\(\\binom{n}{k}\\)\n\n\n\n\n\n\n\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to Probability. Crc Press."
  },
  {
    "objectID": "prob-random-vars.html#basics",
    "href": "prob-random-vars.html#basics",
    "title": "3  Random Variables",
    "section": "3.1 Basics",
    "text": "3.1 Basics\nA random variable, \\(X\\), is a function from the sample space, \\(\\Omega\\), to some subset of \\(\\mathbb{R}\\) with a probability-based rule.\n\\[X: \\Omega \\to \\mathbb R\\]\nRecall that each \\(\\omega \\in \\Omega\\) denotes a state of the world. A random variable \\(X\\) will take take on the value \\(X(\\omega)\\).\nFor example, the event \\(\\{X = 1\\}\\) should be understood the set of of states of the world such that \\(X(\\omega) = 1\\).\n\\[\nP(\\{X = 1\\}) = P(\\{\\omega \\in \\Omega: X(\\omega) = 1\\})\n\\]\nThere are two ways in which to apply functions to random variables:\n\nFunction of a random variable. Use the value of \\(X(\\omega)\\) as input into another function \\(g\\), with the result being another random variable.\n\\[g \\circ X: \\Omega \\to \\mathbb R\\]\nFor example:\n\\[\ng(X) = \\begin{cases} 1 &\\text{if } X>0 \\\\ 0  &\\text{otherwise}\n\\end{cases}\n\\]\nOperator on random variable. These will summarize the properties of random variables such as expectations or variances. We use the \\([\\cdot]\\) notation to denote operators.\n\nWe use uppercase to denote random variables and lowercase to denote particular realizations (or variables in the regular, algebraic sense).\n\nA discrete random variable can only can take on a finite (or countably infinite) number of different values.\nA continuous random variable can take on a continuum of possible values. Loosely speaking, a random variable is continuous if its CDF is continuous."
  },
  {
    "objectID": "prob-random-vars.html#pmfs-pdfs-and-cdfs",
    "href": "prob-random-vars.html#pmfs-pdfs-and-cdfs",
    "title": "3  Random Variables",
    "section": "3.2 PMFs, PDFs, and CDFs",
    "text": "3.2 PMFs, PDFs, and CDFs\nGiven a discrete random variable \\(X\\), we can summarize the probability of each outcome \\(x\\) occurring with a probability mass function (PMF). A continuous random variable is characterized by its probability density function (PDF).\n\\[\n\\underbrace{f(x) = P(X = x)}_\\text{PMF} \\hspace{1cm}\n\\int_a^b \\underbrace{f(x)}_\\text{PDF}dx = P(a \\leq X \\leq b)\n\\]\nNote that both functions must be non-negative\n\\[f(x) \\geq 0 \\hspace{0.5cm} \\text{ for all } x \\in \\mathbb R\\]\nThese functions tell us most of what we need to know about the distribution of random variables (i.e. the complete collection of probabilities assigned to events defined in terms of \\(X\\)).\nThe cumulative distribution function (CDF) tell us everything we need to know about the distribution of random variables. More importantly, a CDF is defined the same way for both discrete and continuous random.\nThe CDF of \\(X\\) is defined as\n\\[F(x) = P(X \\leq x) \\hspace{0.5cm} \\text{ for all } x \\in \\mathbb R\\]\nIn other words, the CDF returns the probability that an outcome for a random variable will be less than or equal to a given value.\nAny CDF \\(F\\) will have the following properties:\n\n\\(F\\) is nondecreasing as \\(x\\) increases\n\n\\[x_1 < x_2 \\to F(x_1) < F(x_2)\\]\n\nLimits\n\n\\[\n\\lim_{x \\to +\\infty} F(x) = 1 \\hspace{0.5cm}\n\\lim_{x \\to -\\infty} F(x) = 0\n\\]\n\nComplement rule\n\n\\[1-F(x) = P(X > x)\\]\n\nContinuity from the right. A CDF is always continuous from the right, even if the random variable is discrete (in which case the CDF is a “step function”).\n\n\\[\\lim_{x \\to a^+} F(x) = F(a)\\]\nIn the case of continuous random variables, note that the PDF is contained inside the definition of the CDF.\n\\[\n\\overbrace{F(a) = P(X \\leq a) = \\int_{-\\infty}^a \\underbrace{f(x)}_\\text{PDF}dx}^\\text{Cumulative Distribution Function}\n\\]\nThat is a probability density is the rate of change in cumulative probability. So where cumulative probability is increasing rapidly, density can easily exceed 1. But if we calculate the area under the density function, it will never exceed 1. In other words, the PDF is a “slope” that is defined according to the Fundamental Theorem of Calculus as follows:\n\\[\\underbrace{f(x) = \\frac{dF(u)}{du} \\bigg|_{u = x}}_\\text{Probability Density Function}\\]\n\nTwo additional definitions.\n\nSupport. . The set of values at which the PMF (or PDF) is positive is called its support.\n\n\\[\n\\textsf{supp}(X) = \\{x \\in \\mathbb R: f(x) > 0\\}\n\\]\n\nThe inverse of a CDF (\\(Q = F^{-1}\\)) is called the quantile function.\nFor example:\n\n\\[\n\\underbrace{Q(0.5)}_\\text{median} = x \\iff P(X \\leq x) = 0.5\n\\]"
  },
  {
    "objectID": "prob-random-vars.html#relationships",
    "href": "prob-random-vars.html#relationships",
    "title": "3  Random Variables",
    "section": "3.3 Relationships",
    "text": "3.3 Relationships\nWhen we say two random variables are equal, we mean that they are equal as functions; they assign the same value to every state of the world.\n\\[\nX = Y \\iff X(\\omega) = Y(\\omega)\\hspace{0.5cm} \\forall \\omega \\in \\Omega\n\\]\nDiscrete multivariate distributions are described by their joint CDF, PMF, or PDF.\n\\[\n\\overbrace{F(a, b) = P(X \\leq a \\cap Y \\leq b) = \\int_{-\\infty}^a \\int_{-\\infty}^b \\underbrace{f(x, y)}_\\text{joint PDF}dy dx}^\\text{Joint Cumulative Distribution Function}\n\\]\nThe same as before, integrating over a PDF will give us probability statements such as\n\\[\nP(a \\leq X \\leq b, c \\leq Y \\leq d) = \\int_a^b \\int_c^d f(x, y)dydx\n\\]\nThe same goes for summing over a PMF:\n\\[\nP(a \\leq X \\leq b, c \\leq Y \\leq d) = \\sum_{x = a}^b \\sum_{y = c}^d f(x, y)\n\\]\nAnd the “slope” interpretation extends to multivariate PDFs:\n\\[\n\\underbrace{f(x, y) = \\frac{\\partial F(u, v)}{\\partial u \\partial v} \\bigg|_{u = x, v = y}}_\\text{Joint Probability Density Function}\n\\]\nAnd for the discrete setting:\n\\[\n\\underbrace{f(x, y) = P(X = x, Y = y)}_\\text{Joint Probability Mass Function}\n\\]\nMarginalization. We can go from multivariate to univariate distributions with summation (for PMFs) or integration (for PDFs). Both of these follow from the law of total probability.\n\nMarginal PMF\n\n\\[\nf_Y(y) = P(Y = y) = \\sum_{\\textsf{supp}[X]} f_{X, Y}(x, y)\n\\]\n\nMarginal PDF\n\n\\[\n\\text{Continuous: }f_Y(y) = \\int_{-\\infty}^\\infty f_{X, Y}(x, y)dx\n\\]\nConditional Distributions. The conditional PMF of \\(Y\\) given \\(X\\) tells us the probability that a given value of \\(Y\\) will occur, given that a certain value of \\(X\\) occurs. In contrast, the conditional PDF of \\(Y\\) given \\(X\\) is just the PDF of \\(Y\\) given that a certain value of \\(X\\) occurs.\n\nConditional PMF\n\n\\[\n\\begin{align}\nf_{Y \\mid X}(y \\mid x) = P(Y = y \\mid X = x) = \\frac{f(x, y)}{f(x)} \\\\\\\\ \\forall y \\in \\mathbb R \\text{ and } \\underbrace{\\forall x \\in \\textsf{supp}(X)}_{\\text{denominator} \\neq 0}\n\\end{align}\n\\]\n\nConditional PDF\n\n\\[\nf_{Y \\mid X}(y \\mid x)  = \\frac{f(x, y)}{f(x)} \\hspace{0.5cm} \\forall y \\in \\mathbb R \\text{ and } \\forall x \\in \\textsf{supp}[X]\n\\]\n\nProduct rule for PMFs and PDFs\n\n\\[\nf(x \\mid y)f(y) = f(x, y)\n\\]\n\nIndependence of random variables regardless of whether they are discrete or continuous.\n\n\\[X \\perp Y \\iff f(x, y) = f(x) f(y)\\]\n\\[X \\perp Y \\iff f(x \\mid y) = f(x)\\]"
  },
  {
    "objectID": "prob-random-vars.html#multivariate-notation",
    "href": "prob-random-vars.html#multivariate-notation",
    "title": "3  Random Variables",
    "section": "3.4 Multivariate Notation",
    "text": "3.4 Multivariate Notation\nA random vector of length \\(K\\) is a vector whose components are random variables:\n\\[\n\\mathbf X (\\omega) = \\pmatrix{X_{[1]} (\\omega), & \\dots, & X_{[K]} (\\omega)}\n\\]\nHere, we use bracketed subscripts to denote distinct random variables because later on we use plain subscripts to denote multiple independent realizations of a single random variable.\nThe use of boldface will make us be able to express complicated expressions in a simple manner.\nFor example:\n\\[\n\\underbrace{F(\\mathbf x)}_\\text{CDF} = P(\\mathbf X \\leq \\mathbf x) = P(X_{[1]} \\leq x_{[1]}, X_{[2]} \\leq x_{[2]}, \\dots, X_{[K]} \\leq x_{[K]})\n\\]\nAnd if we have a continuous random vector, we have the following expression:\n\\[\nF(\\mathbf x) = \\int_{-\\infty}^{x_{[1]}} \\int_{-\\infty}^{x_{[2]}} \\dots \\int_{-\\infty}^{x_{[K]}}  f(u_{[1]}, u_{[2]}, \\dots, u_{[K]})du_{[K]} \\dots du_{[2]}du_{[1]}\n\\]"
  },
  {
    "objectID": "prob-sum-dist.html#summarizing-joint-distributions",
    "href": "prob-sum-dist.html#summarizing-joint-distributions",
    "title": "4  Summarizing Distributions",
    "section": "4.1 Summarizing joint distributions",
    "text": "4.1 Summarizing joint distributions\n\n\n4.1.1 Covariance\nCovariance measures the extent to which two random variables “move together.”\n\\[\n\\begin{align}\n\\text{Cov}[X, Y] &= E\\big[(X - E[X])(Y - E[Y])\\big] \\\\\\\\ &=\nE[XY] - E[X]E[Y]\n\\end{align}\n\\]\n\nVariance Rule (non-linearity of variances)\n\n\\[V[X + Y] = V[X] + 2\\text{Cov}[X, Y] + V[Y]\\]\n\nVariance is a special case of Covariance\n\n\\[\\text{Cov}[X, X] = V[X]\\]\n\nCovariance of sums\n\n\\[\n\\text{Cov}[X + W, Y + Z] = \\text{Cov}[X, Y] + \\text{Cov}[X, Z] + \\text{Cov}[W, Y] + \\text{Cov}[W, Z]\n\\]\nMuch like standard deviation rescales variance, correlation rescales covariance to make its interpretation clearer. The correlation of two random variables is as follows:\n\\[\n\\rho[X, Y] = \\frac{\\text{Cov}[X, Y]}{\\sigma[X] \\sigma[Y]}\n\\]\nThe correlation \\(\\rho\\) is bounded in \\([-1, 1]\\), a fact that derives from the Cauchy-Schwarz inequality.\nLinear dependence describes the relationship between two random variables where one can be written as a linear function of the other. And correlation measures the degree of linear dependence between two random variables.\n\\[\n\\begin{align}\n&\\rho[X, Y] = 1 \\iff Y = a + bX \\\\\\\\\n&\\rho[X, Y] = -1 \\iff Y = a - bX \\\\\\\\\n&\\text{where } b > 0 \\text{ and } a,b \\in \\mathbb R\n\\end{align}\n\\]\nIf two random variables are linearly independent, then \\(\\text{Cov}[X, Y] = 0\\). This fact follows from the definition of covariance and the application of LOTUS.\n\\[\n\\begin{align}\nE[XY] &= \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty x y f(x, y)dydx \\\\\\\\ &=\n\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty xy f_X(x)f_Y(y)dydx \\\\\\\\ &=\n\\int_{-\\infty}^\\infty x f_X(x)dx \\int_{-\\infty}^\\infty y f_Y(y) \\\\\\\\ &=\nE[X]E[Y]\n\\end{align}\n\\]\nThat is, no relationship between two random variables implies no linear relationship them. However, the opposite is not true: lack of correlation does not imply independence.\n\\[\nX \\perp Y \\longrightarrow \\text{Cov}[X, Y] = 0\n\\]\n\n\n4.1.2 Conditional Expectations\nConditional expectations allow us to describe how the “center” of one random variable’s distribution changes once we condition on the observed value of another random variable.\n\nDiscrete case (\\(\\forall x \\in \\textsf{supp}[X]\\))\n\n\\[\nE[Y \\mid X = x] = \\sum_y y f_{Y \\mid X}(y \\mid x)\n\\]\n\nContinuous case (\\(\\forall x \\in \\textsf{supp}[X]\\))\n\n\\[\nE[Y \\mid X = x] = \\int_{-\\infty}^\\infty y f_{Y \\mid X}(y \\mid x)dy\n\\]\nNote that LOTUS can also be applied to conditional expectations of functions of many random variables.\n\\[\nE[g(X, Y) \\mid X = x] = \\int_{-\\infty}^\\infty g(x, y) f_{Y \\mid X}(y \\mid x)dy\n\\]\nUnlike unconditional expectations, \\(E[Y \\mid X = x]\\) is a family of operators on the random vector \\((X, Y)\\) that is indexed by \\(x\\).\nA conditional expectation function (CEF) is just a conditional expectation takes is sometimes denoted by \\(G_Y(x)\\) to emphasize the fact that it’s a function that maps \\(x\\) to \\(E[Y \\mid X = x]\\) rather than the value of \\(E[Y \\mid X = x]\\) at some particular \\(x\\). This notation also emphasizes the fact that it’s a function of \\(x\\), not a function of the random variable \\(Y\\).\nWe write \\(E[X \\mid Y]\\) to denote \\(G_Y(X)\\) which is a function of the random variable of \\(X\\), and thus it’s also a random variable.\nThe CEF is closely linked to various topics such as regression, missing data, and causal inference.\n\nLaw of total expectations (also known as the law of iterated expectations or Adam’s law).\n\\[E[Y] = E\\big[E[Y \\mid X]\\big]\\]\nIt implies that the unconditional expectation can be represented as a weighted average of conditional expectations, where the weights are proportional to the probability distribution of the variable being conditioned on.\nProof\n\n\\[\n\\begin{align}\nE[Y] &= \\sum_y y f_Y(y) \\\\\\\\ &=\n\\sum_y y \\sum_x f_{X, Y}(x, y) \\\\\\\\ &=\n\\sum_x \\sum_y y f_{X \\mid Y}(y \\mid x) f_X(x) \\\\\\\\ &=\n\\underbrace{\\sum_x \\overbrace{\\bigg(\\sum_y y f_{Y \\mid X} (y \\mid x)\\bigg)}^{E[Y \\mid X = x]} f_X (x)}_{E\\big[E[Y \\mid X]\\big]}\n\\end{align}\n\\]\n\nLaw of total variance (also known as Eve’s law).\n\\[V[Y] = \\underbrace{E\\big[V[Y \\mid X]\\big]}_\\text{within-group variation} + \\underbrace{V\\big[E[Y \\mid X]\\big]}_\\text{between-group variation}\\]\nThis theorem allows us to decompose the variability of a random variable \\(Y\\) into the average variability “within” values of \\(X\\) and the variability “across” values of \\(X\\).\nThe ordering of the \\(E\\)’s and \\(V\\)’s spells out \\(EVVE\\), hence the name “Eve’s law”.\nProof.\n\n\\(E[Y] = E\\big[E[Y \\mid X]\\big] = E[G_Y(X)]\\) (Adam’s law)\n\\(E \\big[V[Y \\mid X]\\big] = E\\big[E[Y^2 \\mid X] - G_Y(X)^2\\big] = E\\big[Y^2\\big] - E\\big[G_Y(X)^2\\big]\\)\n\\(V[E[Y \\mid X]] = E\\big[G_Y(X)^2\\big] - E[G_Y(X)]^2 = E\\big[G_Y(X)^2\\big] - E[Y]^2\\)\n\nWe add (2) and (3) to get back to the original definition of \\(V[Y] = E\\big[Y^2\\big] - E[Y]^2\\).\n\nWe can also look at Eve’s law from a different perspective:\n\n“Another way to think about Eve’s law is in terms of prediction. If we wanted to predict someone’s height (\\(Y\\)) based on their age (\\(X\\)) alone, the ideal scenario would be if everyone within an age group had exactly the same height, while different age groups had different heights. Then, given someone’s age, we would be able to predict their height perfectly. In other words, the ideal scenario for prediction is no within-group variation in height, since the within-group variation cannot be explained by age differences. For this reason, within-group variation is also called unexplained variation, and between-group variation is also called explained variation. Eve’s law says that the total variance of \\(Y\\) is the sum of unexplained and explained variation” (Blitzstein & Hwang 2014).\n\n\n\n4.1.3 Best Predictors\nSuppose we knew the full joint cumulative distribution function (CDF) of \\(X\\) and \\(Y\\), and then someone gave us a randomly drawn value of \\(X\\). What would be the guess of \\(Y\\) that would have the lowest MSE? The function \\(g(X)\\) that best approximates \\(Y\\) is the CEF.\n\n\\(E[Y|X]\\), is the best (minimum MSE) predictor of \\(Y\\) given \\(X\\).\n\nThis makes the CEF a natural target of inquiry: if the CEF is known, much is known about how \\(X\\) relates to \\(Y\\). But in many cases the CEF can turn out to be an extremely complicated function.\nWhat if we were to restrict ourselves to just linear functions of the form \\(g_Y(x) = a + b X\\)? We could then define the best linear predictor (BLP) of \\(Y\\) given \\(X\\) as the set of values \\((a, b)\\) that minimizes the MSE.\nThe BLP is expressed as:\n\\[\n(\\alpha, \\beta) = \\underset{a, b \\in \\mathbb R}{\\arg \\min}\\ E\\big[(Y - a + b X)^2\\big]\n\\]\nUsing some calculus, one can show that these values are:\n\\[\n\\begin{align}\n&\\alpha = E[Y] - \\frac{\\text{Cov}[X, Y]}{V[X]} E[X] \\\\\\\\\n&\\beta =  \\frac{\\text{Cov}[X, Y]}{V[X]}\n\\end{align}\n\\]\nNote. This expression is identical to the one provided by the method of ordinary least squares (OLS), which is designed to estimate population parameters from sample data.\nThere are two important corollaries that follow from this:\n\nThe BLP is also the best linear approximation of the CEF.\n\n\\[\n(\\alpha, \\beta) = \\underset{a, b \\in \\mathbb R}{\\arg \\min}\\ E\\bigg[\\big(E[Y \\mid X] - (a + b X)\\big)^2\\bigg]\n\\]\n\nIf the CEF is linear, then the CEF is the BLP.\n\nNote that these are some of the implications of independence for conditional expectations:\n\\[\n\\begin{align}\nY \\perp X \\iff &1.\\ E[Y \\mid X] = E[Y] \\\\\\\\ &2. V[Y \\mid X] = V[Y] \\\\\\\\ &3. \\text{ The BLP of } Y \\text{ given } X \\text{ is } E[Y]\n\\end{align}\n\\]\nPlotting the CEF and BLP\nSuppose we have the following random variables and that we are interested in approximating the CEF \\(G_Y(x)\\) with it’s BLP.\n\\[\n\\begin{align}\n&X \\sim \\text{uniform}(0, 1) \\\\\\\\\n&W \\sim \\text{normal}(0, 1) \\\\\\\\\n&Y = 10X^2 + W\n\\end{align}\n\\]\nBy linearity of expectations, can conclude that the CEF is just\n\\[\nE[Y \\mid X] =  10 X^2\n\\]\nAnd that the BLP (after some algebra) is given by\n\\[\n\\begin{align}\n&\\beta = \\frac{10 \\big(E\\big[X^3\\big] - E[X]E\\big[X^2\\big]\\big)}{E\\big[X^2\\big] - E[X]^2} \\\\\\\\\n&\\alpha = E[10X^2 + W] - \\beta E[X]\n\\end{align}\n\\]\nThe final solution is given by integrating over \\(X \\sim \\text{uniform}(0, 1)\\):\n\\[\n(\\alpha, \\beta) = \\left(-\\frac{5}{3}, 10\\right)\n\\]\nAnd plotting both functions over \\(\\textsf{supp}[X]\\) looks like this:\n\n\nCode\nE <- function(m) {\n  output <- integrate(\n    f = function(x) x^m * fX(x), \n    lower = -Inf, upper = Inf\n    )$value\n}\n\nslope <- function() {\n  num <- 10 * (E(3) - E(1) * E(2))\n  denom <- E(2) - E(1)^2\n  return(num / denom)\n}\n\nintercept <- function() {\n  10 * E(2) - slope() * E(1)\n}\n\nCEF <- function(x) 10*x^2\nfX <- function(x) dunif(x, min = 0, max = 1)\n\ntibble(x = c(-1, 2)) %>% \n  ggplot(aes(x)) +\n  geom_function(fun = CEF, color = \"steelblue\") +\n  geom_function(fun = function(x) intercept() + slope()*x, \n                color = \"tomato\") + \n  geom_rect(aes(xmin = 0, xmax = 1, ymin = -Inf, ymax = Inf), \n            alpha = 1/10)\n\n\n\n\n\nHere, the BLP (in red) approximates the CEF (in blue) reasonably well over the domain of \\(X\\). While this is not always the case, it is very often the case in the social and health sciences. The BLP is thus a good “first approximation” in a very literal sense, in that it is an approximation with a first-order polynomial.\nHowever, when the CEF is not linear, one must take care in interpreting the BLP as equivalent to the CEF. In particular, this may pose a problem when attempting to make inferences where \\(X\\) has low probability mass over some parts of the domain of the CEF. This is because under nonlinearity, the BLP depends on the distribution of \\(X\\).\n\nSee here for an interactive demonstration.\n\n\n\n4.1.4 Properties of residuals\nWe define deviations (or residuals) with the letter \\(\\epsilon\\) and note that they have similar properties when considering with respect to either the CEF or the BLP.\nProperties of deviations\n\n\n\nProperties of deviations\n\n\n\n\n\n\nCEF\nBLP\n\n\n\n\n\\(\\epsilon = Y - E[Y \\mid X]\\)\n\\(\\epsilon = Y - (\\alpha + \\beta X)\\)\n\n\n\\(E[\\epsilon] = 0\\)\n\\(E[\\epsilon] = 0\\)\n\n\n\\(E[\\epsilon \\mid X] = 0\\)\n\\(E[\\epsilon X] = 0\\) (independence)\n\n\n\\(\\text{Cov}[\\epsilon, g(X)] = 0\\)\n\\(\\text{Cov}[\\epsilon, X] = 0\\)\n\n\n\\(V[\\epsilon \\mid X] = V[Y \\mid X]\\)\n\n\n\n\\(V[\\epsilon] = E\\big[V[Y \\mid X]\\big]\\)"
  },
  {
    "objectID": "rn.html",
    "href": "rn.html",
    "title": "Generating Random Numbers",
    "section": "",
    "text": "How do we simulate random numbers from specific probability distributions?\nHow do we use random numbers to do stuff?\n\nThe simplest case involves generating uniform pseudo-random numbers. Methods for generating random numbers from other probability distributions all depend on the uniform random number generator.\nThis set of notes builds upon three black boxes: runif(), sample(), and .Random.seed. It’s no use pretending I know how any of this stuff work.\nADD SOME STUFF FROM THE BOOK I LOANED NICO.\n\nrunif()\nADD EXPLANATION OF THIS.\n\ndunif\n\nfunction (x, min = 0, max = 1, log = FALSE) \n.Call(C_dunif, x, min, max, log)\n<bytecode: 0x7fafa664f610>\n<environment: namespace:stats>\n\npunif\n\nfunction (q, min = 0, max = 1, lower.tail = TRUE, log.p = FALSE) \n.Call(C_punif, q, min, max, lower.tail, log.p)\n<bytecode: 0x7fafa66e1058>\n<environment: namespace:stats>\n\nqunif\n\nfunction (p, min = 0, max = 1, lower.tail = TRUE, log.p = FALSE) \n.Call(C_qunif, p, min, max, lower.tail, log.p)\n<bytecode: 0x7fafa6741928>\n<environment: namespace:stats>\n\nrunif\n\nfunction (n, min = 0, max = 1) \n.Call(C_runif, n, min, max)\n<bytecode: 0x7fafc6b1b158>\n<environment: namespace:stats>\n\n\n\n\nsample()\nR also has a function that allows sampling from finite populations. This sample() function can be used with our without replacement.\n\n## toss six coins\nsample(0:1, size = 6, replace = TRUE)\n\n[1] 1 0 0 0 0 1\n\n## permuation of letters A-Z\nsample(LETTERS) \n\n [1] \"I\" \"K\" \"G\" \"E\" \"C\" \"B\" \"U\" \"J\" \"R\" \"O\" \"H\" \"F\" \"T\" \"X\" \"L\" \"Y\" \"W\" \"M\" \"P\"\n[20] \"S\" \"N\" \"V\" \"A\" \"D\" \"Z\" \"Q\"\n\n## sample from a multinomial distribution\np <- c(0.2, 0.3, 0.5)\nx <- sample(1:3, size = 3000, replace = TRUE, prob = p)\ntable(x)\n\nx\n   1    2    3 \n 604 1003 1393 \n\ntable(x) / length(x)\n\nx\n        1         2         3 \n0.2013333 0.3343333 0.4643333 \n\n\n\n\n.Random.seed\nNote. There’s an integer vector called .Random.seed in the global environment for every R session. This vector changes every time you generate random numbers or every time you change seeds.\nFor example:\n\nset.seed(111)\nstr(.Random.seed)\n\n int [1:626] 10403 624 -762750981 -378653952 -1193945343 -1238812466 800256759 999955148 -1466969763 438919290 ...\n\nset.seed(222)\nstr(.Random.seed)\n\n int [1:626] 10403 624 1488669722 -588037421 -1945880072 -1874066535 1822860934 510535503 495156548 -856964235 ...\n\nrunif(1) ## generate one random number\n\n[1] 0.9315925\n\nstr(.Random.seed)\n\n int [1:626] 10403 1 -2033833551 328541280 -1581583874 -1734191735 219878075 -974258550 -1901840020 939491055 ...\n\nset.seed(111)\nstr(.Random.seed)\n\n int [1:626] 10403 624 -762750981 -378653952 -1193945343 -1238812466 800256759 999955148 -1466969763 438919290 ..."
  },
  {
    "objectID": "rn-methods.html#the-inverse-transform-method",
    "href": "rn-methods.html#the-inverse-transform-method",
    "title": "5  Methods",
    "section": "5.1 The Inverse Transform Method",
    "text": "5.1 The Inverse Transform Method"
  },
  {
    "objectID": "rn-methods.html#the-acceptance-rejection-method",
    "href": "rn-methods.html#the-acceptance-rejection-method",
    "title": "5  Methods",
    "section": "5.2 The Acceptance-Rejection Method",
    "text": "5.2 The Acceptance-Rejection Method"
  },
  {
    "objectID": "rn-methods.html#transformations",
    "href": "rn-methods.html#transformations",
    "title": "5  Methods",
    "section": "5.3 Transformations",
    "text": "5.3 Transformations"
  },
  {
    "objectID": "rn-methods.html#sums-and-mixtures",
    "href": "rn-methods.html#sums-and-mixtures",
    "title": "5  Methods",
    "section": "5.4 Sums and Mixtures",
    "text": "5.4 Sums and Mixtures\nhttp://www.columbia.edu/~ks20/4404-Sigman/4404-Notes-ITM.pdf\nhttps://stephens999.github.io/fiveMinuteStats/inverse_transform_sampling.html\nhttps://en.wikipedia.org/wiki/Inverse_transform_sampling"
  },
  {
    "objectID": "rn-mcmc.html",
    "href": "rn-mcmc.html",
    "title": "7  MCMC",
    "section": "",
    "text": "all from the rizzo book"
  },
  {
    "objectID": "cm.html",
    "href": "cm.html",
    "title": "Correlation Matrices",
    "section": "",
    "text": "This section follows the structure of Hadd and Rodgers (2020).\n\nWhat are correlation matrices? Some math here.\nHow to test correlation matrices? Some null hypothesis testing here.\nHow to model correlation matrices? Some PCA, CFA, and SEM here.\nHow to visualize correlation matrices? This section contains some stuff about geometry and correlation space.\n\n\n\n\n\nHadd, Alexandria, and Joseph Lee Rodgers. 2020. Understanding Correlation Matrices. SAGE Publications."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Blitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to\nProbability. Crc Press.\n\n\nHadd, Alexandria, and Joseph Lee Rodgers. 2020. Understanding\nCorrelation Matrices. SAGE Publications.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in r and Stan. CRC press.\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in\nStatistical Inference. Vol. 26. Springer."
  },
  {
    "objectID": "prob-sum-dist.html",
    "href": "prob-sum-dist.html",
    "title": "4  Summarizing Distributions",
    "section": "",
    "text": "5 Summarizing joint distributions"
  },
  {
    "objectID": "prob-sum-dist.html#expectation",
    "href": "prob-sum-dist.html#expectation",
    "title": "4  Summarizing Distributions",
    "section": "4.1 Expectation",
    "text": "4.1 Expectation\nThe expected value (also known as the expectation or mean) is the most common measure of the “center” of a probability distribution.\n\nDiscrete random variables\n\n\\[\nE[X] = \\sum_{\\textsf{supp}(x)} x f(x)\n\\]\n\nContinuous random variables\n\n\\[\nE[X] = \\int_{-\\infty}^\\infty x f(x)dx\n\\]\n\nProperties of expected values\n\n\\[\n\\begin{align}\n&E[c] = c &\\text{ for all } c \\in \\mathbb R \\\\\\\\\n&E[cX] = c\\ E[X]  &\\text{ for all } c \\in \\mathbb R\n\\end{align}\n\\]\n\nExpectation of a Bernoulli random variable\n\n\\[\nE[X] = P(X = 1) = p\n\\]\nExpectation of a function of a random variable (also known as LOTUS). This comes up in many applications (e.g. finding the variance of a random variable), but the result is far from obvious (see here).\n\nContinuous case\n\n\\[E[g(X)] = \\int_{-\\infty}^\\infty g(x) f_X(x) dx\\]\n\nDiscrete case\n\n\\[E[g(X)] = \\sum_{\\textsf{supp}(x)} g(x) f_X(x)\\]\n\nLinearity of expectations\n\n\\[E[aX + bY + c] = aE[X] + bE[Y] + c\\]\nNote that this property follows from considering the expectation of a function of a bivariate joint distribution.\n\\[\\underbrace{g(X, Y) = aX + bY + c}_\\text{function of a bivariate joint distribution}\\]\nApply LOTUS and marginalization:\n\\[\n\\begin{align}\nE[g(X, Y)] &= \\sum_x \\sum_y g(X, Y) f_{XY}(x,y) \\\\\\\\ &=\n\\sum_x \\sum_y (ax + by + c) f_{XY}(x,y) \\\\\\\\ &=\na\\sum_x \\sum_y x f_{XY}(x,y) +  b\\sum_x \\sum_y y f_{XY}(x,y)+ c\\sum_x \\sum_y f_{XY}(x,y)\n\\end{align}\n\\]"
  },
  {
    "objectID": "prob-sum-dist.html#moments-and-variances",
    "href": "prob-sum-dist.html#moments-and-variances",
    "title": "4  Summarizing Distributions",
    "section": "4.2 Moments and variances",
    "text": "4.2 Moments and variances\nWe can generalize expectations to further characterize the features of a distribution. This is the idea of raw moments, among which the expected value is just a special case.\n\\[\n\\underbrace{\\mu_j^\\prime = E[X^j]}_{j^{th}\\text{ raw moment }}\n\\]\nRaw moments provide summary information about a distribution, describing elements of its shape and location.\n\nCentral moments\n\n\\[\\mu_j = E\\left[(X - E[X])^j\\right]\\]\nThe sole distinction between raw and central moments lies in whether or not the expected value of \\(X\\) is subtracted before calculations.\n\nThe variance (second central moment)\n\n\\[\nV[X] = E\\left[(X - E[X])^2\\right] = E\\left[X^2\\right] - E[X]^2\n\\]\nThe variance measures the expected value of the squared difference between the observed value of \\(X\\) and its mean. Note that the first central moment equals zero.\n\nProperties of variances\n\n\\[\n\\begin{align}\n&V[X + c] = V[X] &\\text{ for all } c \\in \\mathbb R \\\\\\\\\n&V[cX] = c^2\\ E[X]  &\\text{ for all } c \\in \\mathbb R\n\\end{align}\n\\]\n\nStandard deviation\n\n\\[\n\\sigma[X] = \\sqrt{V[X]}\n\\]\nThe standard deviation is often preferable to the variance, since it is on the same scale as the random variable of interest.\nKnowing these two quantities (\\(E[X]\\) and \\(\\sigma[X]\\)) tells everything about normal distributions.\n\nThe normal distribution\n\n\\[X \\sim \\textsf{normal}(\\mu, \\sigma)\\]\n\\[\nE[X] = \\mu \\hspace{0.5cm} \\text{ and } \\hspace{0.5cm}\n\\sigma[X] = \\sigma\n\\]\n\\[\nf_X(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (x - \\mu)^2\\right)\n\\]\nAny linear combination of any number of mutually independent normal random variables must itself be normal.\n\\[\nX \\perp Y \\to X + Y \\sim \\textsf{normal}\\left(\\mu_X + \\mu_Y, \\sqrt{\\sigma_X^2 + \\sigma_Y^2}\\right)\n\\]"
  },
  {
    "objectID": "prob-sum-dist.html#mean-squared-error",
    "href": "prob-sum-dist.html#mean-squared-error",
    "title": "4  Summarizing Distributions",
    "section": "4.3 Mean Squared Error",
    "text": "4.3 Mean Squared Error\nMSE is a metric that characterizes how well a random variable \\(X\\) approximates a certain value \\(c\\).\n\\[MSE = E\\left[(X - c)^2\\right]\\]\nNote that the MSE about zero is the same as the second raw moment, and the MSE about \\(E[X]\\) is the same as the second central moment, which is also the variance.\n\nRoot Mean Squared Error\n\\[\\sqrt{E\\left[(X - c)^2\\right]}\\]\nNote. This is used as a common measure of accuracy in the context of estimation.\nDecomposition\n\n\\[\n\\begin{align}\nE\\left[(X - c)^2\\right] &= E\\left[(X^2 - 2cX + c^2\\right] \\\\\\\\ &=\nE\\left[X^2\\right] - 2cE[X] + c^2 \\\\\\\\ &=\nE\\left[X^2\\right] \\underbrace{- E[X]^2 + E[X]^2}_\\text{clever trick} - 2cE[X] + c^2 \\\\\\\\ &=\n\\left(E\\left[X^2\\right] - E[X]^2\\right) + \\left(E[X]^2 - 2cE[X] + c^2\\right) \\\\\\\\ &=\nV[X] + (E[X]-c)^2\n\\end{align}\n\\]\nNote. In the context of estimation, this is also known as the bias-variance decomposition.\nThe MSE is also linked to an alternative definition of the mean: the value \\(c\\) that minimizes the MSE of \\(X\\) is \\(E[X]\\).\n\\[\n\\underset{c \\in \\mathbb R}{\\arg \\min}\\ E\\left[(X - c)^2\\right] = E[X]\n\\]\nIf we where to choose a different “loss function” besides the MSE, we could come up with different “best” choices for \\(c\\). For example, the median is the value \\(c\\) that minimizes the Mean Absolute Error (\\(|X - c|\\))."
  },
  {
    "objectID": "prob-sum-dist.html#covariance",
    "href": "prob-sum-dist.html#covariance",
    "title": "4  Summarizing Distributions",
    "section": "5.1 Covariance",
    "text": "5.1 Covariance\nCovariance measures the extent to which two random variables “move together.”\n\\[\n\\begin{align}\n\\text{Cov}[X, Y] &= E\\big[(X - E[X])(Y - E[Y])\\big] \\\\\\\\ &=\nE[XY] - E[X]E[Y]\n\\end{align}\n\\]\n\nVariance Rule (non-linearity of variances)\n\n\\[V[X + Y] = V[X] + 2\\text{Cov}[X, Y] + V[Y]\\]\n\nVariance is a special case of Covariance\n\n\\[\\text{Cov}[X, X] = V[X]\\]\n\nCovariance of sums\n\n\\[\n\\text{Cov}[X + W, Y + Z] = \\text{Cov}[X, Y] + \\text{Cov}[X, Z] + \\text{Cov}[W, Y] + \\text{Cov}[W, Z]\n\\]\nMuch like standard deviation rescales variance, correlation rescales covariance to make its interpretation clearer. The correlation of two random variables is as follows:\n\\[\n\\rho[X, Y] = \\frac{\\text{Cov}[X, Y]}{\\sigma[X] \\sigma[Y]}\n\\]\nThe correlation \\(\\rho\\) is bounded in \\([-1, 1]\\), a fact that derives from the Cauchy-Schwarz inequality.\nLinear dependence describes the relationship between two random variables where one can be written as a linear function of the other. And correlation measures the degree of linear dependence between two random variables.\n\\[\n\\begin{align}\n&\\rho[X, Y] = 1 \\iff Y = a + bX \\\\\\\\\n&\\rho[X, Y] = -1 \\iff Y = a - bX \\\\\\\\\n&\\text{where } b > 0 \\text{ and } a,b \\in \\mathbb R\n\\end{align}\n\\]\nIf two random variables are linearly independent, then \\(\\text{Cov}[X, Y] = 0\\). This fact follows from the definition of covariance and the application of LOTUS.\n\\[\n\\begin{align}\nE[XY] &= \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty x y f(x, y)dydx \\\\\\\\ &=\n\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty xy f_X(x)f_Y(y)dydx \\\\\\\\ &=\n\\int_{-\\infty}^\\infty x f_X(x)dx \\int_{-\\infty}^\\infty y f_Y(y) \\\\\\\\ &=\nE[X]E[Y]\n\\end{align}\n\\]\nThat is, no relationship between two random variables implies no linear relationship them. However, the opposite is not true: lack of correlation does not imply independence.\n\\[\nX \\perp Y \\longrightarrow \\text{Cov}[X, Y] = 0\n\\]"
  },
  {
    "objectID": "prob-sum-dist.html#conditional-expectations",
    "href": "prob-sum-dist.html#conditional-expectations",
    "title": "4  Summarizing Distributions",
    "section": "5.2 Conditional Expectations",
    "text": "5.2 Conditional Expectations\nConditional expectations allow us to describe how the “center” of one random variable’s distribution changes once we condition on the observed value of another random variable.\n\nDiscrete case (\\(\\forall x \\in \\textsf{supp}[X]\\))\n\n\\[\nE[Y \\mid X = x] = \\sum_y y f_{Y \\mid X}(y \\mid x)\n\\]\n\nContinuous case (\\(\\forall x \\in \\textsf{supp}[X]\\))\n\n\\[\nE[Y \\mid X = x] = \\int_{-\\infty}^\\infty y f_{Y \\mid X}(y \\mid x)dy\n\\]\nNote that LOTUS can also be applied to conditional expectations of functions of many random variables.\n\\[\nE[g(X, Y) \\mid X = x] = \\int_{-\\infty}^\\infty g(x, y) f_{Y \\mid X}(y \\mid x)dy\n\\]\nUnlike unconditional expectations, \\(E[Y \\mid X = x]\\) is a family of operators on the random vector \\((X, Y)\\) that is indexed by \\(x\\).\nA conditional expectation function (CEF) is just a conditional expectation takes is sometimes denoted by \\(G_Y(x)\\) to emphasize the fact that it’s a function that maps \\(x\\) to \\(E[Y \\mid X = x]\\) rather than the value of \\(E[Y \\mid X = x]\\) at some particular \\(x\\). This notation also emphasizes the fact that it’s a function of \\(x\\), not a function of the random variable \\(Y\\).\nWe write \\(E[X \\mid Y]\\) to denote \\(G_Y(X)\\) which is a function of the random variable of \\(X\\), and thus it’s also a random variable.\nThe CEF is closely linked to various topics such as regression, missing data, and causal inference.\n\nLaw of total expectations (also known as the law of iterated expectations or Adam’s law).\n\\[E[Y] = E\\big[E[Y \\mid X]\\big]\\]\nIt implies that the unconditional expectation can be represented as a weighted average of conditional expectations, where the weights are proportional to the probability distribution of the variable being conditioned on.\nProof\n\n\\[\n\\begin{align}\nE[Y] &= \\sum_y y f_Y(y) \\\\\\\\ &=\n\\sum_y y \\sum_x f_{X, Y}(x, y) \\\\\\\\ &=\n\\sum_x \\sum_y y f_{X \\mid Y}(y \\mid x) f_X(x) \\\\\\\\ &=\n\\underbrace{\\sum_x \\overbrace{\\bigg(\\sum_y y f_{Y \\mid X} (y \\mid x)\\bigg)}^{E[Y \\mid X = x]} f_X (x)}_{E\\big[E[Y \\mid X]\\big]}\n\\end{align}\n\\]\n\nLaw of total variance (also known as Eve’s law).\n\\[V[Y] = \\underbrace{E\\big[V[Y \\mid X]\\big]}_\\text{within-group variation} + \\underbrace{V\\big[E[Y \\mid X]\\big]}_\\text{between-group variation}\\]\nThis theorem allows us to decompose the variability of a random variable \\(Y\\) into the average variability “within” values of \\(X\\) and the variability “across” values of \\(X\\).\nThe ordering of the \\(E\\)’s and \\(V\\)’s spells out \\(EVVE\\), hence the name “Eve’s law”.\nProof.\n\n\\(E[Y] = E\\big[E[Y \\mid X]\\big] = E[G_Y(X)]\\) (Adam’s law)\n\\(E \\big[V[Y \\mid X]\\big] = E\\big[E[Y^2 \\mid X] - G_Y(X)^2\\big] = E\\big[Y^2\\big] - E\\big[G_Y(X)^2\\big]\\)\n\\(V[E[Y \\mid X]] = E\\big[G_Y(X)^2\\big] - E[G_Y(X)]^2 = E\\big[G_Y(X)^2\\big] - E[Y]^2\\)\n\nWe add (2) and (3) to get back to the original definition of \\(V[Y] = E\\big[Y^2\\big] - E[Y]^2\\).\n\nWe can also look at Eve’s law from a different perspective:\n\n“Another way to think about Eve’s law is in terms of prediction. If we wanted to predict someone’s height (\\(Y\\)) based on their age (\\(X\\)) alone, the ideal scenario would be if everyone within an age group had exactly the same height, while different age groups had different heights. Then, given someone’s age, we would be able to predict their height perfectly. In other words, the ideal scenario for prediction is no within-group variation in height, since the within-group variation cannot be explained by age differences. For this reason, within-group variation is also called unexplained variation, and between-group variation is also called explained variation. Eve’s law says that the total variance of \\(Y\\) is the sum of unexplained and explained variation” (Blitzstein & Hwang 2014)."
  },
  {
    "objectID": "prob-sum-dist.html#best-predictors",
    "href": "prob-sum-dist.html#best-predictors",
    "title": "4  Summarizing Distributions",
    "section": "5.3 Best Predictors",
    "text": "5.3 Best Predictors\nSuppose we knew the full joint cumulative distribution function (CDF) of \\(X\\) and \\(Y\\), and then someone gave us a randomly drawn value of \\(X\\). What would be the guess of \\(Y\\) that would have the lowest MSE? The function \\(g(X)\\) that best approximates \\(Y\\) is the CEF.\n\n\\(E[Y|X]\\), is the best (minimum MSE) predictor of \\(Y\\) given \\(X\\).\n\nThis makes the CEF a natural target of inquiry: if the CEF is known, much is known about how \\(X\\) relates to \\(Y\\). But in many cases the CEF can turn out to be an extremely complicated function.\nWhat if we were to restrict ourselves to just linear functions of the form \\(g_Y(x) = a + b X\\)? We could then define the best linear predictor (BLP) of \\(Y\\) given \\(X\\) as the set of values \\((a, b)\\) that minimizes the MSE.\nThe BLP is expressed as:\n\\[\n(\\alpha, \\beta) = \\underset{a, b \\in \\mathbb R}{\\arg \\min}\\ E\\big[(Y - a + b X)^2\\big]\n\\]\nUsing some calculus, one can show that these values are:\n\\[\n\\begin{align}\n&\\alpha = E[Y] - \\frac{\\text{Cov}[X, Y]}{V[X]} E[X] \\\\\\\\\n&\\beta =  \\frac{\\text{Cov}[X, Y]}{V[X]}\n\\end{align}\n\\]\nNote. This expression is identical to the one provided by the method of ordinary least squares (OLS), which is designed to estimate population parameters from sample data.\nThere are two important corollaries that follow from this:\n\nThe BLP is also the best linear approximation of the CEF.\n\n\\[\n(\\alpha, \\beta) = \\underset{a, b \\in \\mathbb R}{\\arg \\min}\\ E\\bigg[\\big(E[Y \\mid X] - (a + b X)\\big)^2\\bigg]\n\\]\n\nIf the CEF is linear, then the CEF is the BLP.\n\nNote that these are some of the implications of independence for conditional expectations:\n\\[\n\\begin{align}\nY \\perp X \\iff &1.\\ E[Y \\mid X] = E[Y] \\\\\\\\ &2. V[Y \\mid X] = V[Y] \\\\\\\\ &3. \\text{ The BLP of } Y \\text{ given } X \\text{ is } E[Y]\n\\end{align}\n\\]\nPlotting the CEF and BLP\nSuppose we have the following random variables and that we are interested in approximating the CEF \\(G_Y(x)\\) with it’s BLP.\n\\[\n\\begin{align}\n&X \\sim \\text{uniform}(0, 1) \\\\\\\\\n&W \\sim \\text{normal}(0, 1) \\\\\\\\\n&Y = 10X^2 + W\n\\end{align}\n\\]\nBy linearity of expectations, can conclude that the CEF is just\n\\[\nE[Y \\mid X] =  10 X^2\n\\]\nAnd that the BLP (after some algebra) is given by\n\\[\n\\begin{align}\n&\\beta = \\frac{10 \\big(E\\big[X^3\\big] - E[X]E\\big[X^2\\big]\\big)}{E\\big[X^2\\big] - E[X]^2} \\\\\\\\\n&\\alpha = E[10X^2 + W] - \\beta E[X]\n\\end{align}\n\\]\nThe final solution is given by integrating over \\(X \\sim \\text{uniform}(0, 1)\\):\n\\[\n(\\alpha, \\beta) = \\left(-\\frac{5}{3}, 10\\right)\n\\]\nAnd plotting both functions over \\(\\textsf{supp}[X]\\) looks like this:\n\n\nCode\nE <- function(m) {\n  output <- integrate(\n    f = function(x) x^m * fX(x), \n    lower = -Inf, upper = Inf\n    )$value\n}\n\nslope <- function() {\n  num <- 10 * (E(3) - E(1) * E(2))\n  denom <- E(2) - E(1)^2\n  return(num / denom)\n}\n\nintercept <- function() {\n  10 * E(2) - slope() * E(1)\n}\n\nCEF <- function(x) 10*x^2\nfX <- function(x) dunif(x, min = 0, max = 1)\n\ntibble(x = c(-1, 2)) %>% \n  ggplot(aes(x)) +\n  geom_function(fun = CEF, color = \"steelblue\") +\n  geom_function(fun = function(x) intercept() + slope()*x, \n                color = \"tomato\") + \n  geom_rect(aes(xmin = 0, xmax = 1, ymin = -Inf, ymax = Inf), \n            alpha = 1/10)\n\n\n\n\n\nHere, the BLP (in red) approximates the CEF (in blue) reasonably well over the domain of \\(X\\). While this is not always the case, it is very often the case in the social and health sciences. The BLP is thus a good “first approximation” in a very literal sense, in that it is an approximation with a first-order polynomial.\nHowever, when the CEF is not linear, one must take care in interpreting the BLP as equivalent to the CEF. In particular, this may pose a problem when attempting to make inferences where \\(X\\) has low probability mass over some parts of the domain of the CEF. This is because under nonlinearity, the BLP depends on the distribution of \\(X\\).\n\nSee here for an interactive demonstration."
  },
  {
    "objectID": "prob-sum-dist.html#properties-of-residuals",
    "href": "prob-sum-dist.html#properties-of-residuals",
    "title": "4  Summarizing Distributions",
    "section": "5.4 Properties of residuals",
    "text": "5.4 Properties of residuals\nWe define deviations (or residuals) with the letter \\(\\epsilon\\) and note that they have similar properties when considering with respect to either the CEF or the BLP.\nProperties of deviations\n\n\n\nProperties of deviations\n\n\n\n\n\n\nCEF\nBLP\n\n\n\n\n\\(\\epsilon = Y - E[Y \\mid X]\\)\n\\(\\epsilon = Y - (\\alpha + \\beta X)\\)\n\n\n\\(E[\\epsilon] = 0\\)\n\\(E[\\epsilon] = 0\\)\n\n\n\\(E[\\epsilon \\mid X] = 0\\)\n\\(E[\\epsilon X] = 0\\) (independence)\n\n\n\\(\\text{Cov}[\\epsilon, g(X)] = 0\\)\n\\(\\text{Cov}[\\epsilon, X] = 0\\)\n\n\n\\(V[\\epsilon \\mid X] = V[Y \\mid X]\\)\n\n\n\n\\(V[\\epsilon] = E\\big[V[Y \\mid X]\\big]\\)"
  },
  {
    "objectID": "prob-transformations.html#ignoring-transformations",
    "href": "prob-transformations.html#ignoring-transformations",
    "title": "5  Transformations",
    "section": "5.1 Ignoring Transformations",
    "text": "5.1 Ignoring Transformations\nLOTUS\nFor simplicity, let’s assume that \\(Y = g(X)\\).\n\\[\n\\underbrace{f_Y(y) = f_X(x) \\left| \\frac{d x}{dy} \\right|}_\\text{Transformation}\n\\]\nWe can then express \\(E[Y]\\) as follows:\n\\[\n\\begin{align}\nE[Y] &= \\int_{-\\infty}^\\infty y\\ f_X(x)\\left| \\frac{d x}{dy} \\right| dy \\\\\\\\ &= \\int_{-\\infty}^\\infty g(x)\\ f_X(x)dx\n\\end{align}\n\\]\nExample: The expectation of a standard log-normal distribution\nSuppose we have the following random variables:\n\n\\(Y = g(X) = e^X\\) and \\(\\frac{dy}{dx} = e^x\\)\n\\(X \\sim \\textsf{normal}(0, 1)\\)\n\nWithout LOTUS. Because \\(f_Y(y)\\) is unknown, we need to first figure it out through the use of transformations. This step can be immensely complicated in other circumnstances.\n\\[f_Y(y) = \\varphi(x) \\left| \\frac{d x}{dy} \\right| = \\varphi(\\log y)\\frac{1}{y}\\]\nWe then obtain the expectation as follows:\n\\[\n\\begin{align}\nE[Y] &= \\int_0^\\infty \\varphi(\\log y) dy \\\\\\\\ &=\n\\int_0^\\infty \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(- \\frac{1}{2} \\log^2 y\\right) dy \\\\\\\\ &= \\sqrt{e}\n\\end{align}\n\\]\nWith LOTUS we don’t have to deal with transformations. Instead, we delve right to it.\n\\[\n\\begin{align}\nE[g(X)] &= \\int_{-\\infty}^\\infty g(x) \\varphi(x) dx \\\\\\\\ &= \\int_{-\\infty}^\\infty  \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(x - \\frac{x^2}{2}\\right) \\\\\\\\ &= \\sqrt{e}\n\\end{align}\n\\]\nThus, LOTUS is quicker because we can ignore transformations. More importantly, this method is less prone to mistakes because we don’t have to worry about changing the bounds on the integrals!"
  }
]